{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rxsDf88BoOPR"
   },
   "source": [
    "# IMPORTS\n",
    "\n",
    "- importing standard modules for data manipulation\n",
    "- importing sklearn tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6IByu01VoIzt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import random\n",
    "import sys\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZZLKcgS1U0DW"
   },
   "outputs": [],
   "source": [
    "# GRIDSEARCH\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# SCORING\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CpbQODLfva4Z"
   },
   "source": [
    "# FUNCTIONS\n",
    "- creating functions to take in data and produce vectorizations / scaling / encodings of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SOMwPVD6xWH"
   },
   "source": [
    "# VECTORIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z8pQfQCXefb3"
   },
   "outputs": [],
   "source": [
    "# let's make functions that return the applied transforms / preprocessing / etc.\n",
    "\n",
    "# TEXT HANDLING\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "\n",
    "# ENCODINGS\n",
    "from sklearn.preprocessing import Binarizer, FunctionTransformer, LabelBinarizer, PolynomialFeatures, RobustScaler\n",
    "\n",
    "# this function takes in a pandas dataframe and the string representation of the column which will be targeted\n",
    "def CntVec(df,target):\n",
    "  # split into X and y datasets\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  # extract numerical and object columns from X dataset\n",
    "  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "  X_num = X_init.select_dtypes(include=numerics)\n",
    "  X_obj = X_init.select_dtypes(include='object')\n",
    "  # fill an empty dataframe with all the vectorizations of the object columns\n",
    "  X_vect = pd.DataFrame()\n",
    "  print('Count vectorizing...')\n",
    "  for col in X_obj.columns:\n",
    "    vect = CountVectorizer(binary=True)\n",
    "    arr = vect.fit_transform(X_obj[col]).toarray()\n",
    "    dfv = pd.DataFrame(arr)\n",
    "    X_vect = pd.concat([X_vect, dfv], axis=1, join_axes=[dfv.index])\n",
    "  # concat the vectorized data and the numeric data\n",
    "  X_prime = pd.concat([X_vect, X_num], axis=1, join_axes=[X_num.index])\n",
    "  # drop any NaNs that may have been made (there were few in the landslides vectorization)\n",
    "  nadrop = pd.concat([X_prime, y_init], axis=1, join_axes=[y_init.index]).dropna()\n",
    "  print('The vectorized data has shape:',nadrop.shape,'\\n')\n",
    "  return nadrop\n",
    "\n",
    "def TfdVec(df,target):\n",
    "  # split into X and y datasets\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  # extract numerical and object columns from X dataset\n",
    "  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "  X_num = df.select_dtypes(include=numerics)\n",
    "  X_obj = df.select_dtypes(include='object')\n",
    "  # fill an empty dataframe with all the vectorizations of the object columns\n",
    "  X_vect = pd.DataFrame()\n",
    "  print('Tfidf vectorizing...')\n",
    "  for col in X_obj.columns:\n",
    "    vect = TfidfVectorizer()\n",
    "    arr = vect.fit_transform(X_obj[col].values.astype('U')).toarray()\n",
    "    df = pd.DataFrame(arr)\n",
    "    X_vect = pd.concat([X_vect, df], axis=1, join_axes=[df.index]).dropna()\n",
    "  # concat the vectorized data and the numeric data\n",
    "  X_prime = pd.concat([X_vect, X_num], axis=1, join_axes=[X_num.index])\n",
    "  # drop any NaNs that may have been made (there were few in the landslides vectorization)\n",
    "  nadrop = pd.concat([X_prime, y_init], axis=1, join_axes=[y_init.index]).dropna()\n",
    "  print('The vectorized data has shape:',nadrop.shape,'\\n')\n",
    "  return nadrop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "18wD9aG76qTm"
   },
   "source": [
    "# ENCODINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9luwVW96cc0"
   },
   "outputs": [],
   "source": [
    "# let's make functions that return the applied transforms / preprocessing / etc.\n",
    "\n",
    "# ENCODINGS\n",
    "from sklearn.preprocessing import Binarizer, FunctionTransformer, LabelBinarizer, PolynomialFeatures, RobustScaler\n",
    "\n",
    "def RobScale(df):\n",
    "  dum = RobustScaler(with_centering=False)\n",
    "  print('Robust fitting...')\n",
    "  fit = dum.fit(df)\n",
    "  print('Robust scaling...')\n",
    "  df2 = fit.transform(df)\n",
    "#   print('Pandas filling...')\n",
    "  dfit = pd.DataFrame(df2).dropna()\n",
    "  print('The scaled data has shape:',dfit.shape,'\\n')\n",
    "  return dfit\n",
    "\n",
    "def Binz(df, target):\n",
    "  # split into X and y datasets\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  dum = Binarizer()\n",
    "  scaled = RobScale(df)\n",
    "  print('Binarizer fitting...')\n",
    "  fit = dum.fit(scaled)\n",
    "  print('Binarizer transforming...')\n",
    "  dfit = pd.DataFrame(fit.transform(scaled))\n",
    "  # drop any NaNs that may have been made (there were few in the landslides vectorization)\n",
    "  dfity = pd.concat([dfit, y_init], axis=1, join_axes=[y_init.index]).dropna()\n",
    "  print('The encoded data has shape:',dfity.shape,'\\n\\n')\n",
    "  return dfity\n",
    "  \n",
    "def FncTran(df, target):\n",
    "  # split into X and y datasets\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  dum = FunctionTransformer()\n",
    "  scaled = RobScale(X_init)\n",
    "  print('Function transformer fitting...')\n",
    "  fit = dum.fit(scaled)\n",
    "  print('Function transforming...')\n",
    "  dfit = pd.DataFrame(fit.transform(scaled))\n",
    "  # drop any NaNs that may have been made (there were few in the landslides vectorization)\n",
    "  dfity = pd.concat([dfit, y_init], axis=1, join_axes=[y_init.index]).dropna()\n",
    "  print('The encoded data has shape:',dfity.shape,'\\n\\n')\n",
    "  return dfity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KkDmgEtgplZ9"
   },
   "source": [
    "# CLASSIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pMVRiWbRl_ZF"
   },
   "outputs": [],
   "source": [
    "# CLASSIFICATIONS\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "\n",
    "### BACKUP LINES FOR ACCURACY SCORE AND CONFUSION\n",
    "#   acc_score = accuracy_score(y_test, pred.predict(X_test))  \n",
    "#   conf_matrix = confusion_matrix(y_test, pred.predict(X_test))\n",
    "#   print('The accuracy score is: \\t\\t%s'%acc_score)\n",
    "#   print('The confusion matrix is:',conf_matrix)\n",
    "\n",
    "# LINEAR REGRESSION\n",
    "def LinReg(df,target):\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_init,y_init,train_size=0.7,random_state=42)\n",
    "  pred = LinearRegression()\n",
    "  pred.fit(X_train, y_train)\n",
    "  msq = mean_squared_error(y_test, pred.predict(X_test))\n",
    "  r2= r2_score(y_test, pred.predict(X_test))\n",
    "  print('The mean squared error is: \\t\\t%s'%msq)\n",
    "  print('The R2 score is: \\t\\t\\t%s'%r2)\n",
    "  return pred\n",
    "  \n",
    "# LOGISTIC REGRESSION\n",
    "def LogReg(df,target):\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_init,y_init,train_size=0.7,random_state=42)\n",
    "  pred = LogisticRegression()\n",
    "  pred.fit(X_train, y_train)\n",
    "  msq = mean_squared_error(y_test, pred.predict(X_test))\n",
    "  r2= r2_score(y_test, pred.predict(X_test))\n",
    "  print('The mean squared error is: \\t\\t%s'%msq)\n",
    "  print('The R2 score is: \\t\\t\\t%s'%r2)\n",
    "  return pred\n",
    "\n",
    "# RIDGE CLASSIFICATION\n",
    "def RidgeClass(df,target):\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_init,y_init,train_size=0.7,random_state=42)\n",
    "  pred = Ridge()\n",
    "  pred.fit(X_train, y_train)\n",
    "  msq = mean_squared_error(y_test, pred.predict(X_test))\n",
    "  r2= r2_score(y_test, pred.predict(X_test))\n",
    "  print('The mean squared error is: \\t\\t%s'%msq)\n",
    "  print('The R2 score is: \\t\\t\\t%s'%r2)\n",
    "  return pred\n",
    "  \n",
    "# LASSO CLASSIFICATION\n",
    "def LassoClass(df,target):\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_init,y_init,train_size=0.7,random_state=42)\n",
    "  pred = Lasso()\n",
    "  pred.fit(X_train, y_train)\n",
    "  msq = mean_squared_error(y_test, pred.predict(X_test))\n",
    "  r2= r2_score(y_test, pred.predict(X_test))\n",
    "  print('The mean squared error is: \\t\\t%s'%msq)\n",
    "  print('The R2 score is: \\t\\t\\t%s'%r2)\n",
    "  return pred\n",
    "  \n",
    "# ELASTICNET CLASSIFICATION\n",
    "def ElastNet(df,target):\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_init,y_init,train_size=0.7,random_state=42)\n",
    "  pred = ElasticNet()\n",
    "  pred.fit(X_train, y_train)\n",
    "  msq = mean_squared_error(y_test, pred.predict(X_test))\n",
    "  r2= r2_score(y_test, pred.predict(X_test))\n",
    "  print('The mean squared error is: \\t\\t%s'%msq)\n",
    "  print('The R2 score is: \\t\\t\\t%s'%r2)\n",
    "  return pred\n",
    "  \n",
    "# DECISION TREE REGRESSOR\n",
    "def TreeReg(df,target):\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_init,y_init,train_size=0.7,random_state=42)\n",
    "  pred = DecisionTreeRegressor()\n",
    "  pred.fit(X_train, y_train)\n",
    "  msq = mean_squared_error(y_test, pred.predict(X_test))\n",
    "  r2= r2_score(y_test, pred.predict(X_test))\n",
    "  print('The mean squared error is: \\t\\t%s'%msq)\n",
    "  print('The R2 score is: \\t\\t\\t%s'%r2)\n",
    "  return pred\n",
    "  \n",
    "# DECISION TREE CLASSIFIER\n",
    "def TreeClass(df,target):\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_init,y_init,train_size=0.7,random_state=42)\n",
    "  pred = DecisionTreeClassifier()\n",
    "  pred.fit(X_train, y_train)\n",
    "  msq = mean_squared_error(y_test, pred.predict(X_test))\n",
    "  r2= r2_score(y_test, pred.predict(X_test))\n",
    "  print('The mean squared error is: \\t\\t%s'%msq)\n",
    "  print('The R2 score is: \\t\\t\\t%s'%r2)\n",
    "  return pred\n",
    "\n",
    "# K NEAREST NEIGHBORS REGRESSOR\n",
    "def KNNReg(df,target):\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_init,y_init,train_size=0.7,random_state=42)\n",
    "  pred = KNeighborsRegressor()\n",
    "  pred.fit(X_train, y_train)\n",
    "  msq = mean_squared_error(y_test, pred.predict(X_test))\n",
    "  r2= r2_score(y_test, pred.predict(X_test))\n",
    "  print('The mean squared error is: \\t\\t%s'%msq)\n",
    "  print('The R2 score is: \\t\\t\\t%s'%r2)\n",
    "  return pred\n",
    "\n",
    "# K NEAREST NEIGHBORS CLASSIFIER\n",
    "def KNNClass(df,target):\n",
    "  X_init = df.drop(target, axis=1)\n",
    "  y_init = df[target]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_init,y_init,train_size=0.7,random_state=42)\n",
    "  pred = KNeighborsClassifier()\n",
    "  pred.fit(X_train, y_train)\n",
    "  msq = mean_squared_error(y_test, pred.predict(X_test))\n",
    "  r2= r2_score(y_test, pred.predict(X_test))\n",
    "  print('The mean squared error is: \\t\\t%s'%msq)\n",
    "  print('The R2 score is: \\t\\t\\t%s'%r2)\n",
    "  return pred "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QhR1OeNHotR5"
   },
   "source": [
    "# SETTING UP THE GPU\n",
    "- want this to run faster\n",
    "- use GPU for rapid simple calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7zzFh6qloLg6"
   },
   "outputs": [],
   "source": [
    "## THIS CODE IS ONLY FOR WORKING IN GOOGLE COLAB ##\n",
    "\n",
    "# import tensorflow as tf\n",
    "# device_name = tf.test.gpu_device_name()\n",
    "# if device_name != '/device:GPU:0':\n",
    "#   raise SystemError('GPU device not found')\n",
    "# print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lwtCcxE8H8Ql"
   },
   "source": [
    "# GETTING THE DATA\n",
    "- We are going to look at a dataset of landslides\n",
    "- This could be interesting, or boring. Not sure yet..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_CtmwWNJyky"
   },
   "outputs": [],
   "source": [
    "import urllib.request, json \n",
    "with urllib.request.urlopen(\"https://data.nasa.gov/resource/tfkf-kniw.json\") as url:\n",
    "    df = pd.DataFrame(json.loads(url.read().decode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiuflZOUPyoi"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admin_division_name</th>\n",
       "      <th>admin_division_population</th>\n",
       "      <th>country_code</th>\n",
       "      <th>country_name</th>\n",
       "      <th>created_date</th>\n",
       "      <th>event_date</th>\n",
       "      <th>event_description</th>\n",
       "      <th>event_id</th>\n",
       "      <th>event_import_id</th>\n",
       "      <th>event_import_source</th>\n",
       "      <th>...</th>\n",
       "      <th>latitude</th>\n",
       "      <th>location_accuracy</th>\n",
       "      <th>location_description</th>\n",
       "      <th>longitude</th>\n",
       "      <th>notes</th>\n",
       "      <th>photo_link</th>\n",
       "      <th>source_link</th>\n",
       "      <th>source_name</th>\n",
       "      <th>storm_name</th>\n",
       "      <th>submitted_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shaanxi</td>\n",
       "      <td>0</td>\n",
       "      <td>CN</td>\n",
       "      <td>China</td>\n",
       "      <td>2017-11-20T15:17:00.000</td>\n",
       "      <td>2008-08-01T00:00:00.000</td>\n",
       "      <td>occurred early in morning, 11 villagers buried...</td>\n",
       "      <td>684</td>\n",
       "      <td>684</td>\n",
       "      <td>glc</td>\n",
       "      <td>...</td>\n",
       "      <td>32.5625</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Sigou Village, Loufan County, Shanxi Province</td>\n",
       "      <td>107.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://blogs.agu.org/landslideblog/2008/10/14...</td>\n",
       "      <td>AGU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-04-01T00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>36619</td>\n",
       "      <td>US</td>\n",
       "      <td>United States</td>\n",
       "      <td>2017-11-20T15:17:00.000</td>\n",
       "      <td>2009-01-02T02:00:00.000</td>\n",
       "      <td>Hours of heavy rain are to blame for an overni...</td>\n",
       "      <td>956</td>\n",
       "      <td>956</td>\n",
       "      <td>glc</td>\n",
       "      <td>...</td>\n",
       "      <td>45.42</td>\n",
       "      <td>5km</td>\n",
       "      <td>Lake Oswego, Oregon</td>\n",
       "      <td>-122.663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.oregonlive.com/news/index.ssf/2009/...</td>\n",
       "      <td>Oregonian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-04-01T00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Junín</td>\n",
       "      <td>14708</td>\n",
       "      <td>PE</td>\n",
       "      <td>Peru</td>\n",
       "      <td>2017-11-20T15:17:00.000</td>\n",
       "      <td>2007-01-19T00:00:00.000</td>\n",
       "      <td>(CBS/AP) At least 10 people died and as many a...</td>\n",
       "      <td>973</td>\n",
       "      <td>973</td>\n",
       "      <td>glc</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.1295</td>\n",
       "      <td>10km</td>\n",
       "      <td>San Ramon district, 195 miles northeast of the...</td>\n",
       "      <td>-75.3587</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.cbsnews.com/news/dozens-missing-af...</td>\n",
       "      <td>CBS News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-04-01T00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mid Western</td>\n",
       "      <td>20908</td>\n",
       "      <td>NP</td>\n",
       "      <td>Nepal</td>\n",
       "      <td>2017-11-20T15:17:00.000</td>\n",
       "      <td>2009-07-31T00:00:00.000</td>\n",
       "      <td>One person was killed in Dailekh district, pol...</td>\n",
       "      <td>1067</td>\n",
       "      <td>1067</td>\n",
       "      <td>glc</td>\n",
       "      <td>...</td>\n",
       "      <td>28.8378</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Dailekh district</td>\n",
       "      <td>81.708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://in.reuters.com/article/idINIndia-41450...</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-04-01T00:00:00.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Central Visayas</td>\n",
       "      <td>798634</td>\n",
       "      <td>PH</td>\n",
       "      <td>Philippines</td>\n",
       "      <td>2017-11-20T15:17:00.000</td>\n",
       "      <td>2010-10-16T12:00:00.000</td>\n",
       "      <td>Another landslide in sitio Bakilid in barangay...</td>\n",
       "      <td>2603</td>\n",
       "      <td>2603</td>\n",
       "      <td>glc</td>\n",
       "      <td>...</td>\n",
       "      <td>10.3336</td>\n",
       "      <td>5km</td>\n",
       "      <td>sitio Bakilid in barangay Lahug</td>\n",
       "      <td>123.8978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.philstar.com/cebu-news/621414/lands...</td>\n",
       "      <td>The Freeman</td>\n",
       "      <td>Supertyphoon Juan (Megi)</td>\n",
       "      <td>2014-04-01T00:00:00.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  admin_division_name admin_division_population country_code   country_name  \\\n",
       "0             Shaanxi                         0           CN          China   \n",
       "1              Oregon                     36619           US  United States   \n",
       "2               Junín                     14708           PE           Peru   \n",
       "3         Mid Western                     20908           NP          Nepal   \n",
       "4     Central Visayas                    798634           PH    Philippines   \n",
       "\n",
       "              created_date               event_date  \\\n",
       "0  2017-11-20T15:17:00.000  2008-08-01T00:00:00.000   \n",
       "1  2017-11-20T15:17:00.000  2009-01-02T02:00:00.000   \n",
       "2  2017-11-20T15:17:00.000  2007-01-19T00:00:00.000   \n",
       "3  2017-11-20T15:17:00.000  2009-07-31T00:00:00.000   \n",
       "4  2017-11-20T15:17:00.000  2010-10-16T12:00:00.000   \n",
       "\n",
       "                                   event_description event_id event_import_id  \\\n",
       "0  occurred early in morning, 11 villagers buried...      684             684   \n",
       "1  Hours of heavy rain are to blame for an overni...      956             956   \n",
       "2  (CBS/AP) At least 10 people died and as many a...      973             973   \n",
       "3  One person was killed in Dailekh district, pol...     1067            1067   \n",
       "4  Another landslide in sitio Bakilid in barangay...     2603            2603   \n",
       "\n",
       "  event_import_source           ...             latitude location_accuracy  \\\n",
       "0                 glc           ...              32.5625           unknown   \n",
       "1                 glc           ...                45.42               5km   \n",
       "2                 glc           ...             -11.1295              10km   \n",
       "3                 glc           ...              28.8378           unknown   \n",
       "4                 glc           ...              10.3336               5km   \n",
       "\n",
       "                                location_description longitude notes  \\\n",
       "0      Sigou Village, Loufan County, Shanxi Province    107.45   NaN   \n",
       "1                                Lake Oswego, Oregon  -122.663   NaN   \n",
       "2  San Ramon district, 195 miles northeast of the...  -75.3587   NaN   \n",
       "3                                   Dailekh district    81.708   NaN   \n",
       "4                    sitio Bakilid in barangay Lahug  123.8978   NaN   \n",
       "\n",
       "  photo_link                                        source_link  source_name  \\\n",
       "0        NaN  https://blogs.agu.org/landslideblog/2008/10/14...          AGU   \n",
       "1        NaN  http://www.oregonlive.com/news/index.ssf/2009/...    Oregonian   \n",
       "2        NaN  https://www.cbsnews.com/news/dozens-missing-af...     CBS News   \n",
       "3        NaN  https://in.reuters.com/article/idINIndia-41450...      Reuters   \n",
       "4        NaN  http://www.philstar.com/cebu-news/621414/lands...  The Freeman   \n",
       "\n",
       "                 storm_name           submitted_date  \n",
       "0                       NaN  2014-04-01T00:00:00.000  \n",
       "1                       NaN  2014-04-01T00:00:00.000  \n",
       "2                       NaN  2014-04-01T00:00:00.000  \n",
       "3                       NaN  2014-04-01T00:00:00.000  \n",
       "4  Supertyphoon Juan (Megi)  2014-04-01T00:00:00.000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCmP0H-xuY-e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKLExcLP1msk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "admin_division_name          object\n",
       "admin_division_population    object\n",
       "country_code                 object\n",
       "country_name                 object\n",
       "created_date                 object\n",
       "event_date                   object\n",
       "event_description            object\n",
       "event_id                     object\n",
       "event_import_id              object\n",
       "event_import_source          object\n",
       "event_title                  object\n",
       "fatality_count               object\n",
       "gazeteer_closest_point       object\n",
       "gazeteer_distance            object\n",
       "injury_count                 object\n",
       "landslide_category           object\n",
       "landslide_setting            object\n",
       "landslide_size               object\n",
       "landslide_trigger            object\n",
       "last_edited_date             object\n",
       "latitude                     object\n",
       "location_accuracy            object\n",
       "location_description         object\n",
       "longitude                    object\n",
       "notes                        object\n",
       "photo_link                   object\n",
       "source_link                  object\n",
       "source_name                  object\n",
       "storm_name                   object\n",
       "submitted_date               object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GYj1VVrNz8dv"
   },
   "outputs": [],
   "source": [
    "# need to handle for converting all possibly numeric data from their original dtype\n",
    "\n",
    "for x in df.columns:\n",
    "  try:\n",
    "    df[x] = df[x].astype('float')\n",
    "  except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAle74KM6uur"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BezEsEakvDuX"
   },
   "outputs": [],
   "source": [
    "# there are a number of columns that will not help us\n",
    "#   - source_link\n",
    "#   - created_date\n",
    "#   - submitted_date\n",
    "#   - photo_link\n",
    "#   - event_description\n",
    "#   - event_id\n",
    "#   - event_import_id\n",
    "#   - event_import_source\n",
    "\n",
    "cols = ['source_link','created_date','submitted_date','photo_link','event_description','event_id','event_import_id','event_import_source','last_edited_date']\n",
    "\n",
    "df = df.drop(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3V-oPJxoN3BS"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OUJssvpSkcBY"
   },
   "source": [
    "# REVIEWING THE DATA\n",
    "- want to see what distributions we have in the dataset for the various fields\n",
    "- different data visualizations and characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOHTfuFSkj2R"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 488,
     "status": "error",
     "timestamp": 1558489655810,
     "user": {
      "displayName": "Mattias Herrfurth",
      "photoUrl": "",
      "userId": "10294660052558722971"
     },
     "user_tz": 240
    },
    "id": "7zonJ4Ikkjzc",
    "outputId": "26cf3feb-863f-4a1d-948f-1b45d9f1f91a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# want to see countplots that mean something to us\n",
    "# get countplots for all columns that have less than 50 unique values\n",
    "\n",
    "for col in df.columns:\n",
    "  if df[col].unique().shape[0] < 50:\n",
    "    sns.countplot(data=df, y=col, palette='Blues_r', order=df[col].value_counts().index)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiHto4bokjwu"
   },
   "outputs": [],
   "source": [
    "# there is only one numeric column with less than 20 unique values\n",
    "\n",
    "# let's see what a scatterplot of the latitude and longitude look like\n",
    "# and add a hue for the gazeteer distance\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "sns.scatterplot(x='longitude',y='latitude',hue='gazeteer_distance',data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-85XgJLJ3-3"
   },
   "outputs": [],
   "source": [
    "# looks vaguely like a world map, as to be expected\n",
    "# why are so many gazeteer_distances == 0.0??\n",
    "\n",
    "# let's use population as a hue\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "sns.scatterplot(x='longitude',y='latitude',hue='admin_division_population',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MR4_zzHfkjtw"
   },
   "outputs": [],
   "source": [
    "# another numeric with many zeroes\n",
    "# i don't know if modeling will work out well...\n",
    "\n",
    "# let's try with fatality count\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "sns.scatterplot(x='longitude',y='latitude',hue='fatality_count',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mU8O2GOHVDO-"
   },
   "outputs": [],
   "source": [
    "# more zeroes, but that's to be expected. most recorded landslides are not fatal.\n",
    "\n",
    "# do fatalities and injuries correlate to size?\n",
    "\n",
    "# going to get means for counts of fatalities/injuries by landslide_size\n",
    "# going to reindex to sort plots by size\n",
    "\n",
    "sizes = ['small','medium','large','very_large','catastrophic']\n",
    "\n",
    "fatal_means = df[df['landslide_size']!='unknown'].groupby('landslide_size')['fatality_count'].mean().reindex(sizes)\n",
    "\n",
    "injure_means = df[df['landslide_size']!='unknown'].groupby('landslide_size')['injury_count'].mean().reindex(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NiAS_fq7aiju"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "fatal_means.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Woecp0dbgyV4"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "injure_means.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXDgXz1mkjnT"
   },
   "outputs": [],
   "source": [
    "# looks like fatalities correlate except for catastrophic\n",
    "# looks like injuries correlate for all sizes\n",
    "\n",
    "# this would help the modeling\n",
    "\n",
    "# alright, enough exploration. let's actually get into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7NcZydYmkjir"
   },
   "outputs": [],
   "source": [
    "# brk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuJiBg7Ov9G5"
   },
   "source": [
    "# LOOKING FOR A TARGET\n",
    "- want to see how many unique values are in columns\n",
    "- the ones with the fewest unique values will be best to use for a target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0qteeMpu5AV"
   },
   "outputs": [],
   "source": [
    "# getting a list of dictionaries for unique counts of all columns\n",
    "# we want to find something to target\n",
    "\n",
    "unq_cnts = []\n",
    "\n",
    "for x in df.columns:\n",
    "  unq_cnts = unq_cnts + [{'cols':x,'cnts':df[x].unique().shape[0]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DE73CBsXwvCQ"
   },
   "outputs": [],
   "source": [
    "unq_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_yz_Ad2IvnYv"
   },
   "outputs": [],
   "source": [
    "# sorting the list, to see if there are targets i can use\n",
    "\n",
    "col_unqs = sorted(unq_cnts, key = lambda i: i['cnts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8rJ3S1tHw0eo"
   },
   "outputs": [],
   "source": [
    "# filtering to only columns with <10 unique elements\n",
    "\n",
    "targ_cols = col_unqs[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kf_V6ArExJvo"
   },
   "outputs": [],
   "source": [
    "targ_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dsEAr7tuxUIa"
   },
   "outputs": [],
   "source": [
    "# looks like we'll have to use landslide_size and landslide_category\n",
    "\n",
    "# lets start with landslide_size\n",
    "\n",
    "print(df.landslide_size.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qRwnZ8uJeZml"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VvgnTtwPwUtZ"
   },
   "source": [
    "# DATA PREPARATION\n",
    "- gotta lose all the NaNs in the target\n",
    "- want to remove \"unknown\" from the target, as well (these aren't helpful)\n",
    "- need to have something to replace the NaNs in data columns\n",
    "- depending on dtype, should use either median for numerics and 'NA' string for objects\n",
    "- need to numerize the target (values 1-5)\n",
    "  - this is good, less unique values is easier for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLy4eDx07zT3"
   },
   "outputs": [],
   "source": [
    "# we'll have to fill nans, remove unknowns in all data columns\n",
    "\n",
    "df = df.dropna(axis=0, subset=['landslide_size'])[df['landslide_size'] != 'unknown']\n",
    "\n",
    "# creating dictonary of what to fill nans with in each column\n",
    "\n",
    "values = {'admin_division_name': 'unknown', 'admin_division_population': df.admin_division_population.median(), 'country_code': 'UNK', \n",
    "          'country_name': 'NA', 'created_date':'2017-11-20T15:17:00.000', 'event_description':'NA', \n",
    "          'fatality_count':df.fatality_count.median(),'gazeteer_closest_point':'NA', \n",
    "          'gazeteer_distance':df.gazeteer_distance.median(), 'injury_count':df.injury_count.median(), 'landslide_setting':'NA', 'landslide_trigger':'NA',\n",
    "          'location_accuracy':'unknown', 'location_description':'NA', 'notes':'NA', 'photo_link':'NA', 'source_link':'NA', 'storm_name':'NA', 'submitted_date':'NA'}\n",
    "\n",
    "df = df.fillna(value=values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9qlAF4vLs6OH"
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DPSO9odaW4NF"
   },
   "outputs": [],
   "source": [
    "# we have to numerize the target\n",
    "\n",
    "mapping = {'small':1, 'medium':2, 'large':3, 'very_large':4, 'catastrophic':5}\n",
    "df = df.replace({'landslide_size':mapping})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcICnGBChuRD"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-g32b5UtymQ"
   },
   "source": [
    "# TESTING FUNCTIONS ON DIFFERENT DATASETS\n",
    "- want to see if the functions that I created would work on multiple datasets\n",
    "- this would prove they can be portable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TXImXEm5t8dN"
   },
   "outputs": [],
   "source": [
    "from seaborn import load_dataset\n",
    "\n",
    "data1 = load_dataset('dots')\n",
    "data2 = load_dataset('fmri')\n",
    "data3 = load_dataset('exercise')\n",
    "data4 = load_dataset('planets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SsxyxZkIxkb4"
   },
   "outputs": [],
   "source": [
    "data1_CntVec = CntVec(data1, 'time')\n",
    "data2_CntVec = CntVec(data2, 'timepoint')\n",
    "data3_CntVec = CntVec(data3, 'kind')\n",
    "data4_CntVec = CntVec(data4, 'method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MdyXkuGm10rg"
   },
   "outputs": [],
   "source": [
    "data2_CntVec.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bamfrUInx60o"
   },
   "outputs": [],
   "source": [
    "# looks like the count vectorization function works for these datasets!\n",
    "\n",
    "# now let's try the binarizing function\n",
    "\n",
    "targ = 'timepoint'\n",
    "\n",
    "data2_Binz = Binz(data2_CntVec, targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nUq7HeRIvFAz"
   },
   "outputs": [],
   "source": [
    "# looks like the binarizing function works for these vectorizations!\n",
    "\n",
    "# now let's try training a logistic regression via the function\n",
    "\n",
    "logreg = LogReg(data2_Binz,targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxIZ5b3HvE-M"
   },
   "outputs": [],
   "source": [
    "# all the ML functions look like they are performing well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8sN67ERxIIy"
   },
   "source": [
    "# BEGIN ANALYZING THE DATA\n",
    "- the functions for ML tools have been tested and are ready for use\n",
    "- need to define target before anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FjeXbrQpdd3"
   },
   "outputs": [],
   "source": [
    "# now that we have the data, we can apply the ML functions to it\n",
    "# we have 2 vectorizations, 2 encodings, and 9 classifications\n",
    "# the order of application for these functions means there are:\n",
    "#    - 4 combinations of vectorizers and encoders\n",
    "#    - 36 total possible combinations of vects/encs/models (4 x 9 = 36)\n",
    "\n",
    "# i tried using vectorizing functions as the input to the transforming functions, but the tuple from the vectorization was read as one input?...\n",
    "\n",
    "target = 'landslide_size'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kw4qRVuLpda0"
   },
   "outputs": [],
   "source": [
    "# vectorizations\n",
    "\n",
    "C_vect = CntVec(df, target)\n",
    "T_vect = TfdVec(df, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v3V63VH1pdYN"
   },
   "outputs": [],
   "source": [
    "# encodings\n",
    "\n",
    "CntFnc = FncTran(C_vect, target)\n",
    "TfdFnc = FncTran(T_vect, target)\n",
    "CntBin = Binz(C_vect, target)\n",
    "TfdBin = Binz(T_vect, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uEZHPwiUpdVg"
   },
   "outputs": [],
   "source": [
    "# # classifications\n",
    "\n",
    "print('\\n\\n**** MODEL = LINEAR REGRESSION *********************')\n",
    "print('\\n\\tFEATURE ENGINEERING = CountVectorizer + FunctionTransformer')\n",
    "CntFncLin = LinReg(CntFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + FunctionTransformer')\n",
    "TfdFncLin = LinReg(TfdFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = CountVectorizer + Binarizer')\n",
    "CntBinLin = LinReg(CntBin,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + Binarizer')\n",
    "TfdBinLin = LinReg(TfdBin,target)\n",
    "\n",
    "print('\\n\\n**** MODEL = LOGISTIC REGRESSION *********************')\n",
    "print('\\n\\tFEATURE ENGINEERING = CountVectorizer + FunctionTransformer')\n",
    "CntFncLog = LogReg(CntFnc,target)\n",
    "#print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + FunctionTransformer')\n",
    "#TfdFncLog = LogReg(TfdFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = CountVectorizer + Binarizer')\n",
    "CntBinLog = LogReg(CntBin,target)\n",
    "#print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + Binarizer')\n",
    "#TfdBinLog = LogReg(TfdBin,target)\n",
    "\n",
    "print('\\n\\n**** MODEL = RIDGE CLASSIFIER *********************')\n",
    "print('\\n\\tFEATURE ENGINEERING = CountVectorizer + FunctionTransformer')\n",
    "CntFncRidge = RidgeClass(CntFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + FunctionTransformer')\n",
    "TfdFncRidge = RidgeClass(TfdFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = CountVectorizer + Binarizer')\n",
    "CntBinRidge = RidgeClass(CntBin,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + Binarizer')\n",
    "TfdBinRidge = RidgeClass(TfdBin,target)\n",
    "\n",
    "print('\\n\\n**** MODEL = LASSO CLASSIFIER *********************')\n",
    "print('\\n\\tFEATURE ENGINEERING = CountVectorizer + FunctionTransformer')\n",
    "CntFncLasso = LassoClass(CntFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + FunctionTransformer')\n",
    "TfdFncLasso = LassoClass(TfdFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = CountVectorizer + Binarizer')\n",
    "CntBinLasso = LassoClass(CntBin,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + Binarizer')\n",
    "TfdBinLasso = LassoClass(TfdBin,target)\n",
    "\n",
    "print('\\n\\n**** MODEL = ELASTIC NET *********************')\n",
    "print('\\n\\tFEATURE ENGINEERING = CountVectorizer + FunctionTransformer')\n",
    "CntFncElast = ElastNet(CntFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + FunctionTransformer')\n",
    "TfdFncElast = ElastNet(TfdFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = CountVectorizer + Binarizer')\n",
    "CntBinElast = ElastNet(CntBin,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + Binarizer')\n",
    "TfdBinElast = ElastNet(TfdBin,target)\n",
    "\n",
    "print('\\n\\n**** MODEL = DECISION TREE REGRESSOR *********************')\n",
    "print('\\n\\tFEATURE ENGINEERING = CountVectorizer + FunctionTransformer')\n",
    "CntFncTreeReg = TreeReg(CntFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + FunctionTransformer')\n",
    "TfdFncTreeReg = TreeReg(TfdFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = CountVectorizer + Binarizer')\n",
    "CntBinTreeReg = TreeReg(CntBin,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + Binarizer')\n",
    "TfdBinTreeReg = TreeReg(TfdBin,target)\n",
    "\n",
    "print('\\n\\n**** MODEL = DECISION TREE CLASSIFIER *********************')\n",
    "print('\\n\\tFEATURE ENGINEERING = CountVectorizer + FunctionTransformer')\n",
    "CntFncTreeClass = TreeClass(CntFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + FunctionTransformer')\n",
    "TfdFncTreeClass = TreeClass(TfdFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = CountVectorizer + Binarizer')\n",
    "CntBinTreeClass = TreeClass(CntBin,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + Binarizer')\n",
    "TfdBinTreeClass = TreeClass(TfdBin,target)\n",
    "\n",
    "print('\\n\\n**** MODEL = KNN REGRESSOR *********************')\n",
    "print('\\n\\tFEATURE ENGINEERING = CountVectorizer + FunctionTransformer')\n",
    "CntFncKNNReg = KNNReg(CntFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + FunctionTransformer')\n",
    "TfdFncKNNReg = KNNReg(TfdFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = CountVectorizer + Binarizer')\n",
    "CntBinKNNReg = KNNReg(CntBin,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + Binarizer')\n",
    "TfdBinKNNReg = KNNReg(TfdBin,target)\n",
    "\n",
    "print('\\n\\n**** MODEL = KNN CLASSIFIER *********************')\n",
    "print('\\n\\tFEATURE ENGINEERING = CountVectorizer + FunctionTransformer')\n",
    "CntFncKNNClass = KNNClass(CntFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + FunctionTransformer')\n",
    "TfdFncKNNClass = KNNClass(TfdFnc,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = CountVectorizer + Binarizer')\n",
    "CntBinKNNClass = KNNClass(CntBin,target)\n",
    "print('\\n\\n\\tFEATURE ENGINEERING = TfidfVectorizer + Binarizer')\n",
    "TfdBinKNNClass = KNNClass(TfdBin,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "72NyJ7TQ-uc_"
   },
   "source": [
    "# Notes on Classifications\n",
    "- the r2 score doesn't help us much unless we are talking about linear/logistic regression\n",
    "- the mean squared error is always helpful, identifying how far our samples are on average from the model\n",
    "\n",
    "# RESULTS\n",
    "- based on the mean squared error we can see that:\n",
    "  - Lasso and ElasticNet models predict with highest accuracy\n",
    "  - the two vectorizations for these two classifications produce Lasso/ENet models with similar accuracy\n",
    "  - the Binarizer produces a slightly more accurate Lasso/ENet model than the FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6mdb6JyWSQe"
   },
   "source": [
    "# GRID SEARCHING\n",
    "- picking the following combination of methods to include in the pipeline:\n",
    "  - CountVectorizer\n",
    "  - Binarizer\n",
    "  - RobustScaler\n",
    "  - ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZjupSeB-Y36"
   },
   "outputs": [],
   "source": [
    "# REFRESHING THE DATA SOURCE\n",
    "\n",
    "import urllib.request, json \n",
    "with urllib.request.urlopen(\"https://data.nasa.gov/resource/tfkf-kniw.json\") as url:\n",
    "    df = pd.DataFrame(json.loads(url.read().decode()))\n",
    "\n",
    "# what if we implemented the vectorization here?...\n",
    "for x in df.columns:\n",
    "  try:\n",
    "    df[x] = df[x].astype('float')\n",
    "  except ValueError:\n",
    "    pass\n",
    "    \n",
    "# we'll have to fill nans, remove unknowns in all data columns\n",
    "df = df.dropna(axis=0, subset=['landslide_size'])[df['landslide_size'] != 'unknown']\n",
    "\n",
    "# creating dictonary of what to fill nans with in each column\n",
    "values = {'admin_division_name': 'unknown', 'admin_division_population': df.admin_division_population.median(), 'country_code': 'UNK', \n",
    "          'country_name': 'NA', 'created_date':'2017-11-20T15:17:00.000', 'event_description':'NA', 'event_import_id': df.event_import_id.mean(), \n",
    "          'event_import_source': 'NA', 'fatality_count':df.fatality_count.median(),'gazeteer_closest_point':'NA', \n",
    "          'gazeteer_distance':df.gazeteer_distance.median(), 'injury_count':df.injury_count.median(), 'landslide_setting':'NA', 'landslide_trigger':'NA',\n",
    "          'location_accuracy':'unknown', 'location_description':'NA', 'notes':'NA', 'photo_link':'NA', 'source_link':'NA', 'storm_name':'NA', 'submitted_date':'NA'}\n",
    "df = df.fillna(value=values)\n",
    "\n",
    "# we have to numerize the target\n",
    "mapping = {'small':1, 'medium':2, 'large':3, 'very_large':4, 'catastrophic':5}\n",
    "df = df.replace({'landslide_size':mapping})\n",
    "\n",
    "# dropping unnecessary columns\n",
    "cols = ['source_link','created_date','submitted_date','photo_link','event_description']\n",
    "df = df.drop(columns=cols)\n",
    "\n",
    "# vectorizing the data\n",
    "df = CntVec(df, 'landslide_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "99aCcoBA5eTd"
   },
   "outputs": [],
   "source": [
    "target = 'landslide_size'\n",
    "\n",
    "X = df.drop(target, axis=1).as_matrix()\n",
    "y = df[target].as_matrix()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zr8jTTlXWRso"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('rs', RobustScaler()),\n",
    "    ('binz', Binarizer()),\n",
    "    ('en', ElasticNet())])\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d3tQlT32j5Ia"
   },
   "outputs": [],
   "source": [
    "# LITTLE SEARCH\n",
    "\n",
    "parameters = {'rs__with_centering':[True],\n",
    "              'rs__with_scaling':[True],\n",
    "#               'rs__copy':[True,False],\n",
    "#               'binz__copy':[True,False],\n",
    "              'binz__threshold':[3.15,3.2,3.25,3.3],\n",
    "              'en__alpha':[0.0,0.5],\n",
    "#               'en__l1_ratio':[0.0,0.25],\n",
    "              'en__fit_intercept':[True,False],\n",
    "              'en__normalize':[True,False],\n",
    "#               'en__precompute':[True,False],\n",
    "#               'en__warm_start':[True,False],\n",
    "#               'en__positive':[True,False]\n",
    "             }\n",
    "\n",
    "search = GridSearchCV(pipeline, parameters, cv=5,n_jobs=-1, verbose=3)\n",
    "search.fit(X_train, y_train)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I1x7tAvTFMVi"
   },
   "outputs": [],
   "source": [
    "# # BIG SEARCH\n",
    "\n",
    "# parameters = {'rs__with_centering':[True],\n",
    "#               'rs__with_scaling':[True],\n",
    "#               'rs__copy':[True,False],\n",
    "#               'binz__copy':[True,False],\n",
    "#               'binz__threshold':[3.15,3.2,3.25,3.3],\n",
    "#               'en__alpha':[0.0,0.5],\n",
    "#               'en__l1_ratio':[0.0,0.25],\n",
    "#               'en__fit_intercept':[True,False],\n",
    "#               'en__normalize':[True,False],\n",
    "#               'en__precompute':[True,False],\n",
    "#               'en__warm_start':[True,False],\n",
    "#               'en__positive':[True,False]\n",
    "#              }\n",
    "\n",
    "# search = GridSearchCV(pipeline, parameters, cv=5,n_jobs=-1, verbose=3)\n",
    "# search.fit(X_train, y_train)\n",
    "# print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "# print(search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "CpbQODLfva4Z",
    "72NyJ7TQ-uc_"
   ],
   "name": "landslides_v21.ipynb",
   "provenance": [
    {
     "file_id": "11C3lyN4Hrd70YvQQu6dc2qeuDVcEJlED",
     "timestamp": 1558109344044
    },
    {
     "file_id": "16jf7G9pi_2Cw1iLelQ0hh--cVF1KRAtN",
     "timestamp": 1557781160299
    },
    {
     "file_id": "1kf9NqOL0-i-7izZ53Km_J74QaqQF_Fc9",
     "timestamp": 1557774240137
    },
    {
     "file_id": "1szv1xtI3js41vIpXk5S-IQcnEN86O5uf",
     "timestamp": 1557770943769
    },
    {
     "file_id": "1JEDqT-UZPRQny1oOgDe4LXkzkTo6kKVF",
     "timestamp": 1557760537277
    },
    {
     "file_id": "1E9evRF1kgygNJbradilsDUDFG5O4VWJz",
     "timestamp": 1557709305126
    },
    {
     "file_id": "1b_Xg1T2jVD3fTSg7UC1wKNTaxNF29qEA",
     "timestamp": 1557707803216
    },
    {
     "file_id": "10fcw3FZhdKG8nPqKApTXdp7gtg2YZJXb",
     "timestamp": 1557705536532
    },
    {
     "file_id": "1Ndzjj6wi6-okTzIlbScYJBtUqq0Id9qx",
     "timestamp": 1557683738945
    },
    {
     "file_id": "19qQOSwdgIlJYXRbvWTDHEsre_JgEUnpI",
     "timestamp": 1557682236232
    },
    {
     "file_id": "1qB2gMjSC_SHSzQ9ATiY1b2j-iGS2hYcb",
     "timestamp": 1557679042220
    },
    {
     "file_id": "1c3s0Bk0KnwD1HnXiTyeFem4SrVWLrfqx",
     "timestamp": 1557677481116
    },
    {
     "file_id": "16neykyKvSsmIatolyrnvl0RBSaMnASxG",
     "timestamp": 1557635936370
    },
    {
     "file_id": "19pZM2SE3puZ5lGm8JAw89pvxBWwQGQlr",
     "timestamp": 1557617865825
    },
    {
     "file_id": "1f_kmPdkWXDkLuuDj63-1gZ2If_GUS-qU",
     "timestamp": 1557615568689
    },
    {
     "file_id": "13UW95ohq7nvZgvdS-CmhYVCCDVAWKeZU",
     "timestamp": 1557595266025
    },
    {
     "file_id": "1VAcUgbiFDpsa8GhS7eJ2If838m6PKN0-",
     "timestamp": 1557587042418
    },
    {
     "file_id": "1hgQ8BiYr-g2sznHBV0jPESvteM3wmQqR",
     "timestamp": 1557584004626
    },
    {
     "file_id": "1ya6byg_lwiGFcMeovDs220exFcgcYfBP",
     "timestamp": 1557450793571
    },
    {
     "file_id": "1s7LjEV-TVDu94nAYZvNYLbgI4Lechbpv",
     "timestamp": 1557273257688
    },
    {
     "file_id": "1wc-OAysyLQqrXF6LdSuxJR8tOFdI8aHT",
     "timestamp": 1557196108302
    },
    {
     "file_id": "1dM8WFYbamd7J0mT9GH3HUbhsaA9BsjYZ",
     "timestamp": 1557104240111
    },
    {
     "file_id": "1PJEZ9Yjjt6KdcWRl4b77KUL-I8h6uGBD",
     "timestamp": 1557011313487
    }
   ],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
