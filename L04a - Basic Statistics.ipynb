{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic Statistics\n",
    "\n",
    "_Including content from the DATA 601 staff, wikipedia, \"Data Science from Scratch\", and elsewhere._\n",
    "\n",
    "A very simple distinction between probability and statistics is that probability is about the _future_ and statistics are about the _past_.\n",
    "\n",
    "More formally...\n",
    "\n",
    "> Probability is primarily a **theoretical branch of mathematics**, which studies the consequences of mathematical definitions. Statistics is primarily an **applied branch of mathematics**, which tries to **make sense of observations in the real world**.\n",
    "\n",
    "from \"Calculated Bets: Computers, Gambling, and Mathematical Modeling to Win\" (http://www3.cs.stonybrook.edu/~skiena/jaialai/excerpts/node12.html)\n",
    "\n",
    "In summary, probability theory enables us to **find the consequences of a given ideal world**, while statistical theory enables us to **measure the extent to which our world is ideal.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have talked about Tukey summaries already, but there are some additional ways to measure spread in our data:  variance and standard deviation.\n",
    "\n",
    "* Variance is defined as ($\\frac{\\sum{(\\bar{x} - x_i)}^{2}}{n-1})$ \n",
    "* Standard deviation is defined as $\\sqrt{variance}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correlation\n",
    "When we talk about correlation, we're looking for the way two values relate to one another.  \n",
    "\n",
    "### Covariance\n",
    "One such approach is _covariance_, $covariance(x, y)$, an analog of variance but for pairs of values.  It does this by computing the dot product of of the _de-mean'ed_ values (the differences from their respective means).\n",
    "\n",
    "A covariance of 0 means no relationship exists.  A \"large\" positive covariance means that x tends to be large when y is large.  Similarly a \"large\" negative covariance means that x tends to be small when y is large and vice versa.\n",
    "\n",
    "Caveats:  Covariance can be difficult to interpret because ...\n",
    "* the units are hard to interpret (since they are the products of the two input sets units)\n",
    "* because the scale of the values is unbounded and their scales are not normalized, knowing what counts as \"large\" is challenging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Correlation\n",
    "Correlation solves this by computing covariance and then dividing by the standard deviation of both variables.\n",
    "\n",
    "The correlation is thus _unitless_ and always lies between -1 (perfect anti-correlation) and 1 (perfect correlation).\n",
    "\n",
    "Caveats:\n",
    "\n",
    "* Correlation can be sensitive to outliers, however, so consider removing them and re-examining correlation.\n",
    "* Simpson's Paradox can cause misinterpretations if the presence of _confounding_ variables isn't accounted for\n",
    "* Correlation looks for _linear_ relationships.  It tells us nothing about non-linear relationships (e.g. $x$ vs. $|x|$)\n",
    "* It also tells us nothing about the magnitude of the relationship, so a perfect positive correlation may be true and yet uninteresting because the effect is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lastly, the classic refrain:  \"correlation does not imply causation\"\n",
    "\n",
    "Note that if two variables are correlated, there are at least three possibilities for why:\n",
    "\n",
    "* x could cause y\n",
    "* y could cause x\n",
    "* some third factor, z, could cause both\n",
    "\n",
    "Randomized trials can help us feel much more secure in a causal assertion.  In general, early on in the EDA process, it's best to avoid causal language (h/t Elements of Data Analytic Style)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simpson's Paradox\n",
    "\n",
    "_taken from \"Data Science from Scratch\"_\n",
    "\n",
    "Often when examining correlation, we come across something called \"Simpson's Paradox\", where the presence of confounding variables isn't accounted for.\n",
    "\n",
    "The key issue is that correlation is measuring the relationship between your two variables **all else being equal**. If your data classes are assigned at random, as they might be in a well-designed experiment, “all else being equal” might not be a terrible assumption. But when there is a deeper pattern to class assignments, “all else being equal” can be an awful assumption.\n",
    "\n",
    "For example, imagine that you can identify all of your members as either East Coast data scientists or West Coast data scientists. You decide to examine which coast’s data scientists are friendlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coast</th>\n",
       "      <th>n_members</th>\n",
       "      <th>avg_friends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West Coast</td>\n",
       "      <td>101</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East Coast</td>\n",
       "      <td>103</td>\n",
       "      <td>6.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        coast  n_members  avg_friends\n",
       "0  West Coast        101          8.2\n",
       "1  East Coast        103          6.5"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "by_coast  = pd.DataFrame({'coast': [\"West Coast\", \"East Coast\"], \n",
    "                          'n_members': [101, 103], \n",
    "                          'avg_friends': [8.2, 6.5]})\n",
    "by_coast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When playing with the data you discover something very strange. If you only look at people with PhDs, the East Coast data scientists have more friends on average. And if you only look at people without PhDs, the East Coast data scientists also have more friends on average!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coast</th>\n",
       "      <th>degree</th>\n",
       "      <th>n_members</th>\n",
       "      <th>avg_friends</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>West Coast</td>\n",
       "      <td>PhD</td>\n",
       "      <td>35</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>East Coast</td>\n",
       "      <td>PhD</td>\n",
       "      <td>70</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>West Coast</td>\n",
       "      <td>no PhD</td>\n",
       "      <td>66</td>\n",
       "      <td>10.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>East Coast</td>\n",
       "      <td>no PhD</td>\n",
       "      <td>33</td>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        coast  degree  n_members  avg_friends\n",
       "0  West Coast     PhD         35          3.1\n",
       "1  East Coast     PhD         70          3.2\n",
       "2  West Coast  no PhD         66         10.9\n",
       "3  East Coast  no PhD         33         13.4"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_coast_and_degree = pd.DataFrame({\n",
    "    'coast': ['West Coast', 'East Coast', 'West Coast', 'East Coast'],\n",
    "    'degree': ['PhD', 'PhD', 'no PhD', 'no PhD'],\n",
    "    'n_members': [35, 70, 66, 33],\n",
    "    'avg_friends': [3.1, 3.2, 10.9, 13.4]\n",
    "})\n",
    "by_coast_and_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once you account for the users’ degrees, the correlation goes in the opposite direction! Bucketing the data as East Coast/West Coast disguised the fact that the East Coast data scientists skew much more heavily toward PhD types."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
