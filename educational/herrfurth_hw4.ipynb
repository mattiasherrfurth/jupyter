{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"herrfurth_hw4.ipynb","version":"0.3.2","provenance":[{"file_id":"1BPrDHd3bMlOn3o_WJksg3z_hhH8a-EU6","timestamp":1551311884842},{"file_id":"1ekkdt4NJRpu3Farh-cUQZmoP3q1eRKWG","timestamp":1536031133585},{"file_id":"1DziSbVqaWErcHDGNfhjupryp5jZFd-t2","timestamp":1535684058223}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"Iqw2pELhpbHS","colab_type":"text"},"cell_type":"markdown","source":["# Naive Bayes Example and SVM Homework"]},{"metadata":{"id":"efA2eNRwPRis","colab_type":"text"},"cell_type":"markdown","source":["Guest Proff: Edward Raff\n","edraff1@umbc.edu\n","raff.edward@umcb.edu\n"]},{"metadata":{"id":"gNktF5vjhozS","colab_type":"code","colab":{}},"cell_type":"code","source":["import pandas as pd\n","from sklearn import datasets\n","import tensorflow as tf\n","from matplotlib import pyplot\n","from matplotlib import pyplot\n","from matplotlib import pylab\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import HashingVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.tree import export_graphviz\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","#some new tricks\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EJL19i9-B-vM","colab_type":"text"},"cell_type":"markdown","source":["Say we have our features $x_1 \\ldots x_d$, and a set of classes $c_1, \\ldots, c_k$. We want to build a classifier based on probability and statistics. Describing a classifier in probablistic terms, we want to know the probability of class $c_k$ given our features $x$, we would write that as:\n","\n","$$P(c_k | x_1, \\ldots, x_d)$$\n","\n","If we knew what the joint probability $P(c, x_1, x_2 ,\\ldots, x_n)$ was, we could apply bayes rule to obtain this answer. \n","\n","$$P(c_k | x_1, \\ldots, x_d)  \\propto P(c_k) \\cdot P(x_1, \\ldots, x_d | c_k) $$\n","\n","\n","How do we evaluate this? One option is to try and factor the joint probability $P(c, x_1, x_2 ,\\ldots, x_n)$. If we factor this using the [chain rule](https://en.wikipedia.org/wiki/Chain_rule_(probability)), we get\n","\n","$$P( x_1, x_2 ,\\ldots, x_d, c) = $$\n","$$P(x_1 | x_2, \\ldots, x_d, c) \\cdot  P(x_2 | x_3, \\ldots, x_d, c) \\cdot  P(x_3 | x_4, \\ldots, x_d, c) \\ldots  P(x_{d-1} | x_d, c) \\cdot  P(x_d | c) \\cdot P(c)$$\n","\n","But this dosn't help us all that much. One naive assumption we could make to simplify things is to assume that knowing the class $c$ explains away all other features. This is the [conditional indepdendence](https://en.wikipedia.org/wiki/Independence_(probability_theory)#Conditional_independence) assumption \n","\n","iff $A\\perp B | C$, then $$P(A , B | C) = P(A | C) \\cdot P(B | C)$$\n","\n","or equivalently, \n","\n","iff $A\\perp B | C$, then $$P(A | B,  C) = P(A | C)$$\n","\n","If we assume $x_i \\perp x_j | C, \\forall i \\neq j$, then we can simplify our application of the chain rule to get\n","\n","$$P(c_k | x_1, \\ldots, x_d) \\propto \\prod_{i=1}^d P(x_i | c_k)$$\n","\n","$$P(c_k | x_1, \\ldots, x_d) = \\frac{\\prod_{i=1}^d P(x_i | c_k)}{\\sum_{z=1}^k \\prod_{i=1}^d P(x_i | c_z)}$$"]},{"metadata":{"id":"2IzlJH50Pr_G","colab_type":"code","outputId":"6e6437f1-c5cb-47aa-adbf-9718b0ed8831","executionInfo":{"status":"ok","timestamp":1551744982966,"user_tz":300,"elapsed":1381,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["# p(pass | eat_pie, study) \n","p_pass = 90.0/100.0\n","#p(pass | eat_pie, study) ∝ p(eat_pie | pass) * p(study | pass) * p(pass)\n","\n","p_study_g_pass = 85.0/90.0\n","p_pie_g_pass = 75/90.0\n","\n","p_study_g_no_pass = 2.0/10.0\n","p_pie_g_no_pass = 5.0/10.0\n","\n","#p_pass_g_pie_study = p_pie_g_pass * p_study_g_pass * p_pass\n","#p_no_pass_g_pie_study = p_pie_g_no_pass * p_study_g_no_pass * (1-p_pass)\n","\n","#What if we remove pie?\n","p_pass_g_pie_study = p_study_g_pass * p_pass\n","p_no_pass_g_pie_study = p_study_g_no_pass * (1-p_pass)\n","\n","print(\"Probability of Passing given that I study and eat pie = \", p_pass_g_pie_study/(p_pass_g_pie_study+p_no_pass_g_pie_study))\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Probability of Passing given that I study and eat pie =  0.9770114942528736\n"],"name":"stdout"}]},{"metadata":{"id":"CIBY2fOPhzCc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"6a1744e9-d439-4a5a-db6c-de03e829652e","executionInfo":{"status":"ok","timestamp":1551744999754,"user_tz":300,"elapsed":18166,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}}},"cell_type":"code","source":["#Load the data\n","data = datasets.fetch_20newsgroups(subset='all')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading 20news dataset. This may take a few minutes.\n","Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"],"name":"stderr"}]},{"metadata":{"id":"N3U9kgaEiH8H","colab_type":"code","outputId":"85e4a990-eb1e-4917-cadc-57d985e7e2ea","executionInfo":{"status":"ok","timestamp":1551744999756,"user_tz":300,"elapsed":18142,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":4177}},"cell_type":"code","source":["print(data.DESCR)"],"execution_count":5,"outputs":[{"output_type":"stream","text":[".. _20newsgroups_dataset:\n","\n","The 20 newsgroups text dataset\n","------------------------------\n","\n","The 20 newsgroups dataset comprises around 18000 newsgroups posts on\n","20 topics split in two subsets: one for training (or development)\n","and the other one for testing (or for performance evaluation). The split\n","between the train and test set is based upon a messages posted before\n","and after a specific date.\n","\n","This module contains two loaders. The first one,\n",":func:`sklearn.datasets.fetch_20newsgroups`,\n","returns a list of the raw texts that can be fed to text feature\n","extractors such as :class:`sklearn.feature_extraction.text.CountVectorizer`\n","with custom parameters so as to extract feature vectors.\n","The second one, :func:`sklearn.datasets.fetch_20newsgroups_vectorized`,\n","returns ready-to-use features, i.e., it is not necessary to use a feature\n","extractor.\n","\n","**Data Set Characteristics:**\n","\n","    =================   ==========\n","    Classes                     20\n","    Samples total            18846\n","    Dimensionality               1\n","    Features                  text\n","    =================   ==========\n","\n","Usage\n","~~~~~\n","\n","The :func:`sklearn.datasets.fetch_20newsgroups` function is a data\n","fetching / caching functions that downloads the data archive from\n","the original `20 newsgroups website`_, extracts the archive contents\n","in the ``~/scikit_learn_data/20news_home`` folder and calls the\n",":func:`sklearn.datasets.load_files` on either the training or\n","testing set folder, or both of them::\n","\n","  >>> from sklearn.datasets import fetch_20newsgroups\n","  >>> newsgroups_train = fetch_20newsgroups(subset='train')\n","\n","  >>> from pprint import pprint\n","  >>> pprint(list(newsgroups_train.target_names))\n","  ['alt.atheism',\n","   'comp.graphics',\n","   'comp.os.ms-windows.misc',\n","   'comp.sys.ibm.pc.hardware',\n","   'comp.sys.mac.hardware',\n","   'comp.windows.x',\n","   'misc.forsale',\n","   'rec.autos',\n","   'rec.motorcycles',\n","   'rec.sport.baseball',\n","   'rec.sport.hockey',\n","   'sci.crypt',\n","   'sci.electronics',\n","   'sci.med',\n","   'sci.space',\n","   'soc.religion.christian',\n","   'talk.politics.guns',\n","   'talk.politics.mideast',\n","   'talk.politics.misc',\n","   'talk.religion.misc']\n","\n","The real data lies in the ``filenames`` and ``target`` attributes. The target\n","attribute is the integer index of the category::\n","\n","  >>> newsgroups_train.filenames.shape\n","  (11314,)\n","  >>> newsgroups_train.target.shape\n","  (11314,)\n","  >>> newsgroups_train.target[:10]\n","  array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\n","\n","It is possible to load only a sub-selection of the categories by passing the\n","list of the categories to load to the\n",":func:`sklearn.datasets.fetch_20newsgroups` function::\n","\n","  >>> cats = ['alt.atheism', 'sci.space']\n","  >>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)\n","\n","  >>> list(newsgroups_train.target_names)\n","  ['alt.atheism', 'sci.space']\n","  >>> newsgroups_train.filenames.shape\n","  (1073,)\n","  >>> newsgroups_train.target.shape\n","  (1073,)\n","  >>> newsgroups_train.target[:10]\n","  array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n","\n","Converting text to vectors\n","~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","In order to feed predictive or clustering models with the text data,\n","one first need to turn the text into vectors of numerical values suitable\n","for statistical analysis. This can be achieved with the utilities of the\n","``sklearn.feature_extraction.text`` as demonstrated in the following\n","example that extract `TF-IDF`_ vectors of unigram tokens\n","from a subset of 20news::\n","\n","  >>> from sklearn.feature_extraction.text import TfidfVectorizer\n","  >>> categories = ['alt.atheism', 'talk.religion.misc',\n","  ...               'comp.graphics', 'sci.space']\n","  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n","  ...                                       categories=categories)\n","  >>> vectorizer = TfidfVectorizer()\n","  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n","  >>> vectors.shape\n","  (2034, 34118)\n","\n","The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\n","components by sample in a more than 30000-dimensional space\n","(less than .5% non-zero features)::\n","\n","  >>> vectors.nnz / float(vectors.shape[0])       # doctest: +ELLIPSIS\n","  159.01327...\n","\n",":func:`sklearn.datasets.fetch_20newsgroups_vectorized` is a function which \n","returns ready-to-use token counts features instead of file names.\n","\n",".. _`20 newsgroups website`: http://people.csail.mit.edu/jrennie/20Newsgroups/\n",".. _`TF-IDF`: https://en.wikipedia.org/wiki/Tf-idf\n","\n","\n","Filtering text for more realistic training\n","~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","It is easy for a classifier to overfit on particular things that appear in the\n","20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\n","high F-scores, but their results would not generalize to other documents that\n","aren't from this window of time.\n","\n","For example, let's look at the results of a multinomial Naive Bayes classifier,\n","which is fast to train and achieves a decent F-score::\n","\n","  >>> from sklearn.naive_bayes import MultinomialNB\n","  >>> from sklearn import metrics\n","  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n","  ...                                      categories=categories)\n","  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n","  >>> clf = MultinomialNB(alpha=.01)\n","  >>> clf.fit(vectors, newsgroups_train.target)\n","  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n","\n","  >>> pred = clf.predict(vectors_test)\n","  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n","  0.88213...\n","\n","(The example :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py` shuffles\n","the training and test data, instead of segmenting by time, and in that case\n","multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\n","yet of what's going on inside this classifier?)\n","\n","Let's take a look at what the most informative features are:\n","\n","  >>> import numpy as np\n","  >>> def show_top10(classifier, vectorizer, categories):\n","  ...     feature_names = np.asarray(vectorizer.get_feature_names())\n","  ...     for i, category in enumerate(categories):\n","  ...         top10 = np.argsort(classifier.coef_[i])[-10:]\n","  ...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\n","  ...\n","  >>> show_top10(clf, vectorizer, newsgroups_train.target_names)\n","  alt.atheism: edu it and in you that is of to the\n","  comp.graphics: edu in graphics it is for and of to the\n","  sci.space: edu it that is in and space to of the\n","  talk.religion.misc: not it you in is that and to of the\n","\n","\n","You can now see many things that these features have overfit to:\n","\n","- Almost every group is distinguished by whether headers such as\n","  ``NNTP-Posting-Host:`` and ``Distribution:`` appear more or less often.\n","- Another significant feature involves whether the sender is affiliated with\n","  a university, as indicated either by their headers or their signature.\n","- The word \"article\" is a significant feature, based on how often people quote\n","  previous posts like this: \"In article [article ID], [name] <[e-mail address]>\n","  wrote:\"\n","- Other features match the names and e-mail addresses of particular people who\n","  were posting at the time.\n","\n","With such an abundance of clues that distinguish newsgroups, the classifiers\n","barely have to identify topics from text at all, and they all perform at the\n","same high level.\n","\n","For this reason, the functions that load 20 Newsgroups data provide a\n","parameter called **remove**, telling it what kinds of information to strip out\n","of each file. **remove** should be a tuple containing any subset of\n","``('headers', 'footers', 'quotes')``, telling it to remove headers, signature\n","blocks, and quotation blocks respectively.\n","\n","  >>> newsgroups_test = fetch_20newsgroups(subset='test',\n","  ...                                      remove=('headers', 'footers', 'quotes'),\n","  ...                                      categories=categories)\n","  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n","  >>> pred = clf.predict(vectors_test)\n","  >>> metrics.f1_score(pred, newsgroups_test.target, average='macro')  # doctest: +ELLIPSIS\n","  0.77310...\n","\n","This classifier lost over a lot of its F-score, just because we removed\n","metadata that has little to do with topic classification.\n","It loses even more if we also strip this metadata from the training data:\n","\n","  >>> newsgroups_train = fetch_20newsgroups(subset='train',\n","  ...                                       remove=('headers', 'footers', 'quotes'),\n","  ...                                       categories=categories)\n","  >>> vectors = vectorizer.fit_transform(newsgroups_train.data)\n","  >>> clf = MultinomialNB(alpha=.01)\n","  >>> clf.fit(vectors, newsgroups_train.target)\n","  MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n","\n","  >>> vectors_test = vectorizer.transform(newsgroups_test.data)\n","  >>> pred = clf.predict(vectors_test)\n","  >>> metrics.f1_score(newsgroups_test.target, pred, average='macro')  # doctest: +ELLIPSIS\n","  0.76995...\n","\n","Some other classifiers cope better with this harder version of the task. Try\n","running :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py` with and without\n","the ``--filter`` option to compare the results.\n","\n",".. topic:: Recommendation\n","\n","  When evaluating text classifiers on the 20 Newsgroups data, you\n","  should strip newsgroup-related metadata. In scikit-learn, you can do this by\n","  setting ``remove=('headers', 'footers', 'quotes')``. The F-score will be\n","  lower because it is more realistic.\n","\n",".. topic:: Examples\n","\n","   * :ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`\n","\n","   * :ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`\n","\n"],"name":"stdout"}]},{"metadata":{"id":"IT9d3Kc3sPSG","colab_type":"code","colab":{}},"cell_type":"code","source":["# For a number of reasons Pandas isn't very useful when processing text data. We'll\n","# stick with lists or numpy arrays.\n","data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n","data_train_y, data_test_y = train_test_split(data.target, random_state=1 )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G-IfEhmfTnT6","colab_type":"code","outputId":"5401ff48-ac8b-479f-bb35-7e281c79f50e","executionInfo":{"status":"ok","timestamp":1551744999761,"user_tz":300,"elapsed":18110,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"cell_type":"code","source":["data_train_X[0]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"From: feustel@netcom.com (David Feustel)\\nSubject: Re: BATF/FBI Murders Almost Everyone in Waco Today! 4/19\\nOrganization: DAFCO: OS/2 Software Support & Consulting\\nLines: 10\\n\\nIt's truly unfortunate that we don't have the Japanese tradition of\\nHari-Kari for public officials to salvage some tatters of honor after\\nthey commit offenses against humanity like were perpetrated in Waco,\\nTexas today.\\n-- \\nDave Feustel N9MYI <feustel@netcom.com>\\n\\nI'm beginning to look forward to reaching the %100 allocation of taxes\\nto pay for the interest on the national debt. At that point the\\nfederal government will be will go out of business for lack of funds.\\n\""]},"metadata":{"tags":[]},"execution_count":7}]},{"metadata":{"id":"i6ISgoxxnA7v","colab_type":"code","colab":{}},"cell_type":"code","source":["# Here we use SKLearn's CountVectorizer\n","# note we use fit transform on the training data and transform on the test data\n","# if a word only appears in the test data it is ignored.\n","vectorizer = CountVectorizer(binary=False)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"riKEvuYvU3X9","colab_type":"code","outputId":"18e2380e-74b8-4044-a91e-9591143dbc87","executionInfo":{"status":"ok","timestamp":1551745005152,"user_tz":300,"elapsed":23479,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["vectors_train[0]"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<1x153196 sparse matrix of type '<class 'numpy.int64'>'\n","\twith 80 stored elements in Compressed Sparse Row format>"]},"metadata":{"tags":[]},"execution_count":9}]},{"metadata":{"id":"0DKeQjR4WcXQ","colab_type":"code","outputId":"5bc1f15c-58ef-4041-9741-8bf1f1f7fb21","executionInfo":{"status":"ok","timestamp":1551745005153,"user_tz":300,"elapsed":23459,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["word_of_interest = \"re\"\n","#Did email # 0 have the word \"from\"\n","print(\"Did email 0 have the word '\", word_of_interest, \"': \", vectors_train[0][0,vectorizer.vocabulary_[word_of_interest]] >= 1)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Did email 0 have the word ' re ':  True\n"],"name":"stdout"}]},{"metadata":{"id":"xYIH9jLziNkp","colab_type":"code","outputId":"4dc3dbd2-2acd-4ca1-83c8-ec443b91d83d","executionInfo":{"status":"ok","timestamp":1551745005154,"user_tz":300,"elapsed":23440,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"cell_type":"code","source":["# Let's look at the shape of the data\n","# The first number is the number of examples\n","# The second is the number of words in the corpus\n","print(vectors_train.shape)\n","print(vectors_test.shape)\n"],"execution_count":11,"outputs":[{"output_type":"stream","text":["(14134, 153196)\n","(4712, 153196)\n"],"name":"stdout"}]},{"metadata":{"id":"nGi3vARobGRK","colab_type":"code","outputId":"8d1d8557-6a49-462b-8b5c-bb1004bafa91","executionInfo":{"status":"ok","timestamp":1551745005155,"user_tz":300,"elapsed":23424,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":924}},"cell_type":"code","source":["# Let's look at the shape of the data\n","# (document, token)     1 if present (Sparse representation so if it's not there 0 implied)\n","print(vectors_train)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["  (0, 67564)\t1\n","  (0, 87663)\t1\n","  (0, 42700)\t1\n","  (0, 106908)\t1\n","  (0, 70259)\t1\n","  (0, 38642)\t1\n","  (0, 145948)\t2\n","  (0, 70625)\t1\n","  (0, 64650)\t1\n","  (0, 111510)\t1\n","  (0, 36019)\t1\n","  (0, 54023)\t1\n","  (0, 101517)\t1\n","  (0, 105838)\t1\n","  (0, 79938)\t1\n","  (0, 108926)\t1\n","  (0, 133280)\t1\n","  (0, 33106)\t1\n","  (0, 2647)\t1\n","  (0, 117412)\t1\n","  (0, 66645)\t1\n","  (0, 90355)\t1\n","  (0, 38905)\t1\n","  (0, 101164)\t1\n","  (0, 53599)\t1\n","  :\t:\n","  (14133, 69349)\t2\n","  (14133, 42728)\t1\n","  (14133, 83751)\t1\n","  (14133, 127516)\t1\n","  (14133, 144750)\t1\n","  (14133, 70572)\t2\n","  (14133, 134323)\t1\n","  (14133, 33821)\t1\n","  (14133, 80748)\t3\n","  (14133, 146276)\t1\n","  (14133, 33677)\t1\n","  (14133, 36019)\t2\n","  (14133, 105838)\t1\n","  (14133, 127776)\t1\n","  (14133, 135553)\t8\n","  (14133, 105264)\t2\n","  (14133, 134355)\t8\n","  (14133, 145072)\t1\n","  (14133, 81002)\t3\n","  (14133, 89567)\t1\n","  (14133, 131382)\t1\n","  (14133, 106436)\t1\n","  (14133, 130634)\t2\n","  (14133, 48888)\t3\n","  (14133, 67189)\t3\n"],"name":"stdout"}]},{"metadata":{"id":"i5mBr5kHCGcS","colab_type":"text"},"cell_type":"markdown","source":["The above equation is much easier to deal with. We can now deal with each feature independently, and simply multiply them together! The only qestion is, how do we pick $P$? We need to define a probability distribution that we belive each $x_i$ comes from. \n","\n","For text classification problems, the multinomial distribution is the most popular choice to use with naive bayes. "]},{"metadata":{"id":"s4__x4mrjEFi","colab_type":"code","outputId":"3c6a31f6-e65e-4a2a-9d54-3a6dd3915c39","executionInfo":{"status":"ok","timestamp":1551745005466,"user_tz":300,"elapsed":23720,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["nb = MultinomialNB(alpha=.1)\n","nb.fit(vectors_train, data_train_y)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)"]},"metadata":{"tags":[]},"execution_count":13}]},{"metadata":{"id":"QngUJdxJYByr","colab_type":"code","outputId":"986c95e2-2235-42a7-dbc3-fd8ebe02ff6b","executionInfo":{"status":"ok","timestamp":1551745005590,"user_tz":300,"elapsed":23826,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":746}},"cell_type":"code","source":["#print a confusion matrix and accuracy\n","print(confusion_matrix(nb.predict(vectors_test), data_test_y))\n","print(accuracy_score(nb.predict(vectors_test), data_test_y))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[[183   0   1   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0\n","    0  16]\n"," [  0 220  43   5   7  29   1   2   0   0   0   3   7   1   2   1   0   1\n","    0   0]\n"," [  0   0  39   0   2   0   1   0   0   0   0   0   0   0   0   0   1   0\n","    0   0]\n"," [  0   7 101 232   9   5   5   0   0   0   0   0  13   2   0   1   1   0\n","    0   0]\n"," [  0   3  25  11 209   1   4   0   1   0   0   0   5   0   1   1   0   1\n","    0   0]\n"," [  0   5  30   1   0 189   0   0   0   0   0   2   0   0   0   0   0   0\n","    0   0]\n"," [  1   2   5   5   2   1 213   3   4   1   0   1   0   2   1   0   0   0\n","    0   0]\n"," [  1   0   3   1   0   0  12 241   1   0   0   0   2   0   0   1   0   0\n","    0   1]\n"," [  0   0   2   0   0   1   2  14 244   0   1   0   1   2   0   1   0   0\n","    0   0]\n"," [  0   0   0   0   0   0   2   0   0 221   4   0   0   0   2   0   0   0\n","    0   1]\n"," [  0   0   0   1   0   0   1   0   0   1 255   0   0   0   0   1   0   0\n","    0   0]\n"," [  0   2   3   0   0   1   2   0   0   1   0 248   0   1   0   0   1   1\n","    2   0]\n"," [  0   2   7   6   4   1  10   4   1   2   0   0 200   3   2   0   0   0\n","    0   0]\n"," [  0   0   0   0   0   0   2   0   0   0   0   0   0 231   1   3   0   0\n","    0   0]\n"," [  0   1   0   0   0   2   0   0   0   0   0   0   0   4 247   0   0   0\n","    2   0]\n"," [  3   0   0   0   0   0   0   0   0   0   0   0   0   0   0 231   0   1\n","    1   9]\n"," [  0   0   0   0   0   0   2   1   0   0   1   1   0   1   0   0 225   1\n","   19   4]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0 219\n","    3   0]\n"," [  0   0   3   0   1   0   0   0   0   0   1   2   0   2   0   0   6   0\n","  167   3]\n"," [ 11   0   1   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n","    3  98]]\n","0.8726655348047538\n"],"name":"stdout"}]},{"metadata":{"id":"seFhBmYQCYXY","colab_type":"text"},"cell_type":"markdown","source":["# What about continuous data?\n","\n","You have to use the \"right\" (read, good enough) distribution for the type of data you are looking at. When we are working with non-text data, the Multinomial is not a popular choice, and people tend to use what is called the Gaussian Naive Bayes variant. \n","\n","In this case, we assume $P(x_i | c_j)$  follows a guassian distribution. Since this one is easier, we will work through the math. We essenitaly just plug in the guassian distribution to get the following likelihood. \n","\n","$$P \\left( x _ { i } | c_j \\right) = \\frac { 1 } { \\sqrt { 2 \\pi \\sigma _ { c_j } ^ { 2 } } } \\exp \\left( - \\frac { \\left( x _ { i } - \\mu _ { c_j } \\right) ^ { 2 } } { 2 \\sigma _ { c_j } ^ { 2 } } \\right)$$\n","\n","\n","Now we have two parameters of the guassian distribution $\\mu_{c_j}$ and $\\sigma{c_j}$ that we need to set. If we wanted to take an optimization approach and define an objective function, we would take the negative log likelihood\n","\n","$$-\\log\\left(P(x_i | c_j)\\right) = \\frac{1}{2} \\left(\\frac{(\\mu_{c_j}-x_i)^2}{{\\sigma _ { c_j }}^2}+2 \\log (\\sigma _ { c_j })+\\log (2)+\\log (\\pi )\\right)$$\n","\n","\n","Lucky for us, guassians are convient, and the minimizer is in the notation $\\mu_{ c_j }$ is the mean of $x_i$ in class $c_j$ and $\\sigma _ { c_j }$ is the standard deviation of $x_i$ for class $c_j$. If $n_{c_j}$ is the number of data points that have the label $c_j$, we get\n","\n","$$\\mu_{c_j} = \\frac{1}{n_{c_j}} \\sum_{z=1}^{n_{c_j}} x^{(z)}_i$$\n","\n","$$\\sigma_{c_j} = \\sqrt{\\frac{1}{n_{c_j}}  \\sum_{z=1}^{n_{c_j}} \\left(\\mu_{c_j}-x^{(z)}_i\\right)^2}$$\n","\n","Now lets try it out on some data!"]},{"metadata":{"id":"wVQOU5OLCTSd","colab_type":"code","outputId":"c9cde914-1cc2-41c6-9fa5-30c311f88e97","executionInfo":{"status":"ok","timestamp":1551745005592,"user_tz":300,"elapsed":23805,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["data_iris = datasets.load_iris()\n","data_iris_train_X, data_test_X = train_test_split(data_iris.data, random_state=1 )\n","data_iris_train_y, data_test_y = train_test_split(data_iris.target, random_state=1 )\n","#gnb= GaussianNB()\n","gnb= MultinomialNB()\n","gnb.fit(data_iris_train_X, data_iris_train_y)"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"id":"wcI7_szCDB1s","colab_type":"code","outputId":"3d6b8fa4-bdf7-414b-9092-2ba6232aa043","executionInfo":{"status":"ok","timestamp":1551745005594,"user_tz":300,"elapsed":23792,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":88}},"cell_type":"code","source":["pred_iris=gnb.predict(data_test_X)\n","print( confusion_matrix( pred_iris, data_test_y ) )\n","print( accuracy_score( pred_iris, data_test_y ) )"],"execution_count":16,"outputs":[{"output_type":"stream","text":["[[13  0  0]\n"," [ 0  0  0]\n"," [ 0 16  9]]\n","0.5789473684210527\n"],"name":"stdout"}]},{"metadata":{"id":"pqFCAkTyeMMw","colab_type":"text"},"cell_type":"markdown","source":["# Homework Problem 1\n","- Note the data contains many headers, quotes and headers.  These create things that are easy to idenitify. You can remove this from the documents using  data_train = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes')\n","- What happens to the performance of the model?  Is this to be expected?  Was it a good idea?\n","- I used a CountVectorizer(binary=True)\n","- What happens to the vectors if you set binary=False?  What happens to the performance?\n","- Try training an svm with a linear kernel using binary vectors, count vectors, also apply a tfidf transform to the data.  Compare performance with Naive Bayes.\n","\n","\n"]},{"metadata":{"id":"pekQtaQ1eFS_","colab_type":"code","outputId":"9a40d33d-2932-4ab5-b5ea-ce913028fcb3","executionInfo":{"status":"ok","timestamp":1551745011666,"user_tz":300,"elapsed":29830,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":746}},"cell_type":"code","source":["import pandas as pd\n","from sklearn import datasets\n","import tensorflow as tf\n","from matplotlib import pyplot\n","from matplotlib import pyplot\n","from matplotlib import pylab\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import HashingVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.tree import export_graphviz\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","#some new tricks\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","\n","# getting 20newsgroups dataset\n","data = datasets.fetch_20newsgroups(subset='all')\n","\n","data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n","data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n","\n","# vectorizer = CountVectorizer(binary=False)\n","vectorizer = CountVectorizer(binary=True)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vectors_train, data_train_y)\n","\n","#print a confusion matrix and accuracy\n","print(confusion_matrix(nb.predict(vectors_test), data_test_y))\n","print(accuracy_score(nb.predict(vectors_test), data_test_y))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[[184   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0\n","    0  17]\n"," [  0 220  36   5   3  22   0   3   0   0   0   7   7   2   1   2   0   0\n","    0   0]\n"," [  0   0  81   0   2   0   1   0   0   0   0   0   0   0   0   0   1   0\n","    0   0]\n"," [  0   7  97 231  10   6   8   0   0   2   0   1   9   3   1   1   0   0\n","    0   0]\n"," [  0   3   6  10 211   0   4   0   1   0   0   0   6   0   1   1   0   0\n","    0   0]\n"," [  0   4  23   1   0 194   0   0   0   0   1   2   0   0   1   0   0   0\n","    0   0]\n"," [  1   1   2   6   3   1 214   4   4   2   0   0   1   1   1   0   0   0\n","    0   0]\n"," [  1   0   3   0   0   0  10 240   1   0   0   0   2   1   0   0   0   0\n","    0   1]\n"," [  0   0   0   0   0   1   3  10 244   0   1   0   0   0   0   0   0   0\n","    0   0]\n"," [  0   0   0   0   0   0   2   0   0 221   2   0   0   0   1   0   0   0\n","    0   0]\n"," [  0   0   0   1   0   0   1   0   0   1 255   0   0   0   0   1   0   0\n","    0   0]\n"," [  0   2   5   0   1   1   2   1   0   0   0 244   1   0   0   0   2   0\n","    0   0]\n"," [  0   4   5   8   4   2   8   3   1   0   1   0 202   3   3   0   0   0\n","    0   0]\n"," [  0   0   0   0   0   1   1   2   0   0   0   0   0 233   0   5   0   1\n","    1   0]\n"," [  0   0   2   0   0   2   1   0   0   0   0   0   0   4 247   0   0   0\n","    1   0]\n"," [  3   0   0   0   0   0   0   0   0   0   0   0   0   0   0 228   0   1\n","    1   8]\n"," [  0   0   0   0   0   0   2   2   0   0   1   1   0   0   0   0 225   1\n","   16   4]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1 220\n","    2   0]\n"," [  0   1   2   0   0   0   0   0   0   0   1   2   1   2   0   1   5   1\n","  174   3]\n"," [ 10   0   1   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0\n","    2  99]]\n","0.8843378607809848\n"],"name":"stdout"}]},{"metadata":{"id":"QdwLlVh2fm2R","colab_type":"code","outputId":"6f97372b-3ff1-47de-fe3e-e1a3b3baf99e","executionInfo":{"status":"ok","timestamp":1551745019273,"user_tz":300,"elapsed":37424,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":746}},"cell_type":"code","source":["# getting cleaned data for training\n","data_clean = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n","\n","data_clean_train_X, data_test_X = train_test_split(data_clean.data, random_state=1 )\n","data_clean_train_y, data_test_y = train_test_split(data_clean.target, random_state=1 )\n","\n","# vect_clean = CountVectorizer(binary=False)\n","vect_clean = CountVectorizer(binary=True)\n","vect_clean_train=vect_clean.fit_transform(data_train_X)\n","vect_clean_test=vect_clean.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vect_clean_train, data_train_y)\n","\n","#print a confusion matrix and accuracy\n","print(confusion_matrix(nb.predict(vect_clean_test), data_test_y))\n","print(accuracy_score(nb.predict(vect_clean_test), data_test_y))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["[[138   1   2   1   1   1   0   2   3   0   3   3   0   2   2   7   4   5\n","    9  21]\n"," [  1 181  38   4   2  27   3   2   0   0   0   6   8   3   7   2   0   2\n","    0   0]\n"," [  0   0  34   0   3   0   0   0   0   0   0   0   0   0   0   0   1   0\n","    0   0]\n"," [  0  11 104 219  17   8  14   0   2   2   0   0  14   2   2   0   0   0\n","    0   0]\n"," [  0  12  14   9 180   4  10   0   2   0   1   1   8   1   1   1   0   1\n","    1   0]\n"," [  0  10  37   2   2 175   0   1   0   0   1   2   0   0   2   0   0   1\n","    0   0]\n"," [  0   1   2   4   4   2 179   6   3   4   0   0   3   2   0   0   0   0\n","    0   1]\n"," [  2   0   0   2   4   1  15 214  16   0   1   0   4   2   3   0   1   0\n","    1   2]\n"," [  3   1   4   0   0   1   1   6 191   1   3   1   2   2   0   1   2   1\n","    2   0]\n"," [  5   7   9   7   8   2   4  12   9 209  13  11   7   7  10   5   9   6\n","    5   5]\n"," [  0   1   1   0   0   0   3   0   1   1 226   0   0   0   1   0   0   0\n","    1   0]\n"," [  4   5   4   2   2   5   5   1   1   1   1 203   4   0   3   0   8   2\n","    1   0]\n"," [  0   5   5   8   7   0  14   6   4   0   1   3 171   2   3   1   1   0\n","    0   1]\n"," [  1   0   0   2   2   0   0   2   2   2   1   2   0 209   2   2   0   0\n","    7   1]\n"," [  0   2   1   0   0   2   1   1   3   0   0   3   3   3 205   1   1   0\n","    1   1]\n"," [ 22   2   4   0   2   0   3   1   1   1   3   3   0   6   3 214   1   2\n","    3  39]\n"," [  5   1   0   1   0   1   3   5   6   2   4   8   2   1   1   2 185   4\n","   32   8]\n"," [  3   1   0   0   0   0   0   1   1   0   1   0   0   1   2   2   2 191\n","    3   1]\n"," [  3   1   2   1   0   0   2   5   4   3   2   9   0   6   7   3  13   9\n","  126   3]\n"," [ 12   0   2   0   0   1   0   0   2   0   1   2   3   0   2   2   6   0\n","    5  49]]\n","0.7425721561969439\n"],"name":"stdout"}]},{"metadata":{"id":"4otPkpWljjj6","colab_type":"text"},"cell_type":"markdown","source":["# RESULT\n","... it looks like it got worse?"]},{"metadata":{"id":"3UWWIO8skFi_","colab_type":"text"},"cell_type":"markdown","source":["* I used a CountVectorizer(binary=True)\n","* What happens to the vectors if you set binary=False? What happens to the performance?\n","* Try training an svm with a linear kernel using binary vectors, count vectors, also apply a tfidf transform to the data. Compare performance with Naive Bayes.\n","\n","\n"]},{"metadata":{"id":"XohRvjW8kN0k","colab_type":"code","outputId":"5e6bf09f-1109-4895-b3b0-61605974affb","executionInfo":{"status":"ok","timestamp":1551745026938,"user_tz":300,"elapsed":45075,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":746}},"cell_type":"code","source":["# getting cleaned data for training\n","data_clean = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n","\n","data_clean_train_X, data_test_X = train_test_split(data_clean.data, random_state=1 )\n","data_clean_train_y, data_test_y = train_test_split(data_clean.target, random_state=1 )\n","\n","vect_false = CountVectorizer(binary=False)\n","vect_false_train=vect_false.fit_transform(data_train_X)\n","vect_false_test=vect_false.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vect_false_train, data_train_y)\n","\n","#print a confusion matrix and accuracy\n","print(confusion_matrix(nb.predict(vect_false_test), data_test_y))\n","print(accuracy_score(nb.predict(vect_false_test), data_test_y))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[[135   0   2   1   1   0   0   3   2   1   6   2   1   1   6  10   5   7\n","    9  27]\n"," [  1 186  45   4   4  31   4   2   1   2   0   3   9   1   6   3   0   2\n","    0   0]\n"," [  0   0   8   0   2   0   0   0   0   0   0   0   0   0   0   0   1   0\n","    0   0]\n"," [  0   8 104 210  16   5  11   0   2   0   0   0  12   2   0   0   0   0\n","    0   0]\n"," [  0  10  17  12 174   4  11   0   2   0   1   1   8   1   1   1   0   0\n","    0   0]\n"," [  0  10  44   2   3 174   0   1   0   0   0   3   0   0   1   0   0   1\n","    0   0]\n"," [  0   0   2   5   4   1 174   3   2   3   0   0   1   1   0   0   0   0\n","    0   0]\n"," [  1   0   0   1   4   2  15 217  16   0   1   1   5   2   3   0   1   0\n","    1   2]\n"," [  3   1   1   0   0   1   3   9 190   1   1   1   1   2   2   1   0   0\n","    0   0]\n"," [  5   7  10   7   7   1   4  12   9 206  17  11   7   6   9   5   7   5\n","    5   5]\n"," [  0   0   1   0   0   0   1   0   1   1 222   1   0   0   0   0   0   0\n","    1   0]\n"," [  2   7  10   2   2   3   6   0   1   0   0 200   7   0   2   0   7   3\n","    0   0]\n"," [  0   3   5  11  10   1  15   4   3   1   0   3 167   2   3   1   1   0\n","    0   0]\n"," [  1   1   1   1   3   1   1   1   3   2   1   1   1 215   2   0   0   0\n","    5   1]\n"," [  1   2   1   0   0   2   1   0   2   1   0   3   2   1 202   1   2   0\n","    2   1]\n"," [ 22   2   4   2   2   2   2   2   0   1   4   2   0   4   3 210   0   4\n","    4  36]\n"," [  5   2   2   2   0   1   4   4   6   1   3   7   3   1   1   1 178   3\n","   31   9]\n"," [  5   1   0   0   0   0   0   1   2   2   1   3   0   2   3   4   5 191\n","    3   1]\n"," [  4   2   5   2   1   0   5   6   7   4   4  14   1   8  10   3  21   7\n","  131   4]\n"," [ 14   0   1   0   1   1   0   0   2   0   1   1   4   0   2   3   6   1\n","    5  46]]\n","0.7292020373514432\n"],"name":"stdout"}]},{"metadata":{"id":"bMxynHhRlBdI","colab_type":"text"},"cell_type":"markdown","source":["# RESULT\n","... it looks like it got worse again????\n","\n","Let's try it on the original..."]},{"metadata":{"id":"QG-bTeyLlQAz","colab_type":"code","outputId":"954a4444-97c6-4f51-941d-238d80f1ec68","executionInfo":{"status":"ok","timestamp":1551745032929,"user_tz":300,"elapsed":51050,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":746}},"cell_type":"code","source":["# getting 20newsgroups dataset\n","data = datasets.fetch_20newsgroups(subset='all')\n","\n","data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n","data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n","\n","vectorizer = CountVectorizer(binary=False)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vectors_train, data_train_y)\n","\n","#print a confusion matrix and accuracy\n","print(confusion_matrix(nb.predict(vectors_test), data_test_y))\n","print(accuracy_score(nb.predict(vectors_test), data_test_y))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["[[183   0   1   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0\n","    0  16]\n"," [  0 220  43   5   7  29   1   2   0   0   0   3   7   1   2   1   0   1\n","    0   0]\n"," [  0   0  39   0   2   0   1   0   0   0   0   0   0   0   0   0   1   0\n","    0   0]\n"," [  0   7 101 232   9   5   5   0   0   0   0   0  13   2   0   1   1   0\n","    0   0]\n"," [  0   3  25  11 209   1   4   0   1   0   0   0   5   0   1   1   0   1\n","    0   0]\n"," [  0   5  30   1   0 189   0   0   0   0   0   2   0   0   0   0   0   0\n","    0   0]\n"," [  1   2   5   5   2   1 213   3   4   1   0   1   0   2   1   0   0   0\n","    0   0]\n"," [  1   0   3   1   0   0  12 241   1   0   0   0   2   0   0   1   0   0\n","    0   1]\n"," [  0   0   2   0   0   1   2  14 244   0   1   0   1   2   0   1   0   0\n","    0   0]\n"," [  0   0   0   0   0   0   2   0   0 221   4   0   0   0   2   0   0   0\n","    0   1]\n"," [  0   0   0   1   0   0   1   0   0   1 255   0   0   0   0   1   0   0\n","    0   0]\n"," [  0   2   3   0   0   1   2   0   0   1   0 248   0   1   0   0   1   1\n","    2   0]\n"," [  0   2   7   6   4   1  10   4   1   2   0   0 200   3   2   0   0   0\n","    0   0]\n"," [  0   0   0   0   0   0   2   0   0   0   0   0   0 231   1   3   0   0\n","    0   0]\n"," [  0   1   0   0   0   2   0   0   0   0   0   0   0   4 247   0   0   0\n","    2   0]\n"," [  3   0   0   0   0   0   0   0   0   0   0   0   0   0   0 231   0   1\n","    1   9]\n"," [  0   0   0   0   0   0   2   1   0   0   1   1   0   1   0   0 225   1\n","   19   4]\n"," [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0 219\n","    3   0]\n"," [  0   0   3   0   1   0   0   0   0   0   1   2   0   2   0   0   6   0\n","  167   3]\n"," [ 11   0   1   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n","    3  98]]\n","0.8726655348047538\n"],"name":"stdout"}]},{"metadata":{"id":"Rxyks3DllXyA","colab_type":"text"},"cell_type":"markdown","source":["# RESULT\n","Also worse, but not by much (0.8843 down to 0.8727)"]},{"metadata":{"id":"4zEIUQ6hluW5","colab_type":"text"},"cell_type":"markdown","source":["* Try training an svm with a linear kernel using binary vectors, count vectors, also apply a tfidf transform to the data. Compare performance with Naive Bayes."]},{"metadata":{"id":"LAoZRulIlwhB","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"fjZhs1lZ1vjI","colab_type":"code","outputId":"0f37b817-7f71-4118-d1f0-b8416f0a841e","executionInfo":{"status":"ok","timestamp":1551745372405,"user_tz":300,"elapsed":390505,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":746}},"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# getting 20newsgroups dataset\n","data = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n","\n","data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n","data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n","\n","vectorizer = TfidfVectorizer(binary=True)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","svm = SVC(kernel='linear')\n","svm.fit(vectors_train, data_train_y)\n","\n","#print a confusion matrix and accuracy\n","print(confusion_matrix(svm.predict(vectors_test), data_test_y))\n","print(accuracy_score(svm.predict(vectors_test), data_test_y))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["[[129   1   0   2   0   0   0   3   2   3   2   2   1   2   4  12   3   5\n","    8  23]\n"," [  1 167  11   7   5  21   2   1   4   3   1   8   5   2   5   1   1   0\n","    1   0]\n"," [  1  11 181  17  10  18   4   0   0   0   0   2   3   0   2   1   1   0\n","    1   0]\n"," [  0   9  20 196  18   3   6   0   0   0   0   2  10   1   0   0   0   1\n","    0   1]\n"," [  1   7   8  10 166   1   4   1   1   0   1   1   6   2   0   1   1   0\n","    0   0]\n"," [  0  10  11   1   1 170   2   1   0   0   1   2   0   0   1   1   1   0\n","    0   0]\n"," [  1   2   0   5   6   0 204   6   4   2   0   1   4   1   1   0   0   0\n","    0   1]\n"," [  6  10  14  11   9   6  12 208  17  19  11  16  13   8  12   8   9   5\n","    9   9]\n"," [  6   3   3   0   4   2   4  14 195   7   4   1   6   4   3   0   6   4\n","    4   3]\n"," [  5   1   0   0   1   2   1   0   4 176  13   2   1   4   2   2   1   6\n","    5   2]\n"," [  0   0   0   0   0   0   0   0   2   9 215   0   0   0   0   0   1   0\n","    1   0]\n"," [  1   3   2   0   1   0   2   0   1   0   1 185   3   1   2   0   5   1\n","    2   0]\n"," [  2   7   8  11   8   4   8  12   4   0   1   9 164   7   6   2   1   0\n","    2   1]\n"," [  2   2   2   0   1   1   2   5   2   0   0   0   3 205   1   4   1   0\n","    4   3]\n"," [  4   2   2   0   2   2   1   2   6   3   5   6   5   5 207   4   5   3\n","    2   4]\n"," [ 16   3   0   0   0   0   0   0   1   0   0   1   0   0   0 190   1   1\n","    2  33]\n"," [  1   2   0   0   0   0   4   6   3   2   3   8   1   2   2   3 174   3\n","   37   8]\n"," [  5   1   0   1   0   0   0   2   1   1   2   3   1   0   2   5   1 185\n","    2   3]\n"," [  3   0   1   1   1   0   1   2   3   1   2   7   2   5   6   4  18   9\n","  111   3]\n"," [ 15   1   0   0   1   0   0   2   1   0   0   1   1   0   0   5   4   1\n","    6  38]]\n","0.7355687606112055\n"],"name":"stdout"}]},{"metadata":{"id":"OEh38X8FE-7r","colab_type":"text"},"cell_type":"markdown","source":["# RESULT\n","* This actually took really long, and after researching a bit on the web it seems this is to be expected...\n","* It looks like I did something wrong here, though"]},{"metadata":{"id":"88cMu4BBeEIY","colab_type":"text"},"cell_type":"markdown","source":["# Homework Problem 2\n","- Use the fetch_sms_spam() function below to get the sms spam data set.\n","- More info: [Here](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)\n","- Apply the vectorization and classification tasks above to  the data.\n","- Which gave the best performance (use a confusionmatrix and accuraccy score to help decide this)."]},{"metadata":{"id":"q93BVO8XMHnQ","colab_type":"code","colab":{}},"cell_type":"code","source":["# It's worth while reading through this function there's useful things here, that\n","# I'm not explicitly covering in class.\n","def fetch_sms_spam():\n","  import requests # requests is a handy http library\n","  import zipfile # a zip library\n","  r=requests.get('http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/smsspamcollection.zip')\n","  # I know of no way to get data out of a zipfile without saving to disk extracting and reading in, sigh\n","  zf=open('smsspam.zip','wb')\n","  zf.write(r.content)\n","  zip_ref = zipfile.ZipFile('smsspam.zip', 'r')\n","  zip_ref.extractall('smsspam')\n","  zip_ref.close()\n","  zf.close\n","  sms_file=open('smsspam/SMSSpamCollection.txt','r')\n","  #object to return\n","  data = {'data':[], 'target':[], 'target_classes': ['ham', 'spam']}\n","  # First tab splits the class and the SMS message\n","  # There's an argument to be made I should use the csv library and use\n","  # delimiter = '\\t', I can't argue with that - it's a good idea. I did it this\n","  # way for pedogogy.\n","  for line in sms_file:\n","    idx = line.find('\\t')\n","    target = line[:idx]\n","    doc = line[idx+1:]\n","    data['data'] += [doc]\n","    if target == 'ham': data['target'] += [0]\n","    else: data['target'] += [1]\n","  sms_file.close()\n","  return data"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ROJZrR_kt_IJ","colab_type":"code","colab":{}},"cell_type":"code","source":["data1 = fetch_sms_spam()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rEo8XyHQxHb-","colab_type":"code","outputId":"6066542f-4063-424c-ea6e-b33503136cf1","executionInfo":{"status":"ok","timestamp":1551745377790,"user_tz":300,"elapsed":395866,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n","data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n","\n","vectorizer = CountVectorizer(binary=True)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","svm = SVC()\n","svm.fit(vectors_train, data_train_y)\n","\n","#print a confusion matrix and accuracy\n","print(confusion_matrix(svm.predict(vectors_test), data_test_y))\n","print(accuracy_score(svm.predict(vectors_test), data_test_y))"],"execution_count":24,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[[1212  182]\n"," [   0    0]]\n","0.8694404591104734\n"],"name":"stdout"}]},{"metadata":{"id":"RPGi3vZd97NP","colab_type":"text"},"cell_type":"markdown","source":["# RESULT\n","\n","* Not entirely sure what to make of that...\n","\n","* Accuracy score looks decent, at least."]},{"metadata":{"id":"2gbFu-o0_opf","colab_type":"code","outputId":"afe41679-0566-44db-8a30-9b42866b31d5","executionInfo":{"status":"ok","timestamp":1551745379398,"user_tz":300,"elapsed":397462,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":126}},"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n","data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n","\n","vectorizer = CountVectorizer(binary=False)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","svm = SVC()\n","svm.fit(vectors_train, data_train_y)\n","\n","#print a confusion matrix and accuracy\n","print(confusion_matrix(svm.predict(vectors_test), data_test_y))\n","print(accuracy_score(svm.predict(vectors_test), data_test_y))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n","  \"avoid this warning.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[[1212  182]\n"," [   0    0]]\n","0.8694404591104734\n"],"name":"stdout"}]},{"metadata":{"id":"TE6iDWJV_vKQ","colab_type":"text"},"cell_type":"markdown","source":["# RESULT\n","\n","* vectors being binary or not doesn't matter for SVC"]},{"metadata":{"id":"5fM5Lgxc-HvP","colab_type":"code","outputId":"3a532b73-ff2c-469a-8198-d1bfb3899151","executionInfo":{"status":"ok","timestamp":1551745379556,"user_tz":300,"elapsed":397608,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"cell_type":"code","source":["data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n","data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n","\n","vectorizer = CountVectorizer(binary=False)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vectors_train, data_train_y)\n","\n","#print a confusion matrix and accuracy\n","print(confusion_matrix(nb.predict(vectors_test), data_test_y))\n","print(accuracy_score(nb.predict(vectors_test), data_test_y))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["[[1204    7]\n"," [   8  175]]\n","0.9892395982783357\n"],"name":"stdout"}]},{"metadata":{"id":"yBmz9a7p-eFA","colab_type":"text"},"cell_type":"markdown","source":["# RESULT\n","* Again, confusion matrix is a bit odd...\n","* But accuracy score is way better with Naive Bayes!!"]},{"metadata":{"id":"n1jDDEZC-p0I","colab_type":"code","outputId":"e20e303c-8e1e-48ca-f4ac-45b9d18dd575","executionInfo":{"status":"ok","timestamp":1551745379695,"user_tz":300,"elapsed":397737,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"cell_type":"code","source":["vectorizer = CountVectorizer(binary=True)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vectors_train, data_train_y)\n","\n","#print a confusion matrix and accuracy\n","print(confusion_matrix(nb.predict(vectors_test), data_test_y))\n","print(accuracy_score(nb.predict(vectors_test), data_test_y))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["[[1203    7]\n"," [   9  175]]\n","0.9885222381635581\n"],"name":"stdout"}]},{"metadata":{"id":"i7klzvIn-5__","colab_type":"text"},"cell_type":"markdown","source":["# RESULT\n","* nearly identical accuracy score no matter if binary or not\n","\n","# CONCLUSION\n","\n","* out of previous tests of various prediction models, it was found that the accuracy scores are as follow:\n","  * SVC = 0.8694\n","  * NB w/ Binary Vectors = 0.9885\n","  * NB w/o Binary Vectors = 0.9892\n","* It looks like the Naive Bayes without Binary Vectors performed the best for the spam detection data!\n","* The following code produces only the accuracy score for all the models created in this assignment"]},{"metadata":{"id":"lkp9m12FI7rh","colab_type":"code","outputId":"58699c5c-cea7-4d30-d69b-babd8bcdc30b","executionInfo":{"status":"ok","timestamp":1551745411958,"user_tz":300,"elapsed":429983,"user":{"displayName":"Mattias Herrfurth","photoUrl":"","userId":"10294660052558722971"}},"colab":{"base_uri":"https://localhost:8080/","height":177}},"cell_type":"code","source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sun Mar  3 14:43:55 2019\n","\n","@author: matti\n","\"\"\"\n","\n","# =============================================================================\n","# Naive Bayes with binary vectors as applied to the unclean 20newsgroups data\n","# =============================================================================\n","# getting 20newsgroups dataset\n","data = datasets.fetch_20newsgroups(subset='all')\n","\n","data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n","data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n","\n","# vectorizer = CountVectorizer(binary=False)\n","vectorizer = CountVectorizer(binary=True)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vectors_train, data_train_y)\n","\n","#print the accuracy score\n","print('The accuracy score for the Naive Bayes with binary vectors as applied to the unclean 20newsgroups data is: ',accuracy_score(nb.predict(vectors_test), data_test_y))\n","\n","\n","# =============================================================================\n","# Naive Bayes with binary vectors as applied to the cleaned 20newsgroups data without headers, footers, and quotes\n","# =============================================================================\n","# getting cleaned data for training\n","data_clean = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n","\n","data_clean_train_X, data_test_X = train_test_split(data_clean.data, random_state=1 )\n","data_clean_train_y, data_test_y = train_test_split(data_clean.target, random_state=1 )\n","\n","# vect_clean = CountVectorizer(binary=False)\n","vect_clean = CountVectorizer(binary=True)\n","vect_clean_train=vect_clean.fit_transform(data_train_X)\n","vect_clean_test=vect_clean.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vect_clean_train, data_train_y)\n","\n","#print the accuracy score\n","print('The accuracy score for the Naive Bayes with binary vectors as applied to the cleaned 20newsgroups data is: ',accuracy_score(nb.predict(vect_clean_test), data_test_y))\n","\n","\n","# =============================================================================\n","# Naive Bayes without binary vectors as applied to the unclean 20newsgroups data\n","# =============================================================================\n","# getting 20newsgroups dataset\n","data = datasets.fetch_20newsgroups(subset='all')\n","\n","data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n","data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n","\n","vectorizer = CountVectorizer(binary=False)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vectors_train, data_train_y)\n","\n","#print the accuracy score\n","print('The accuracy score for the Naive Bayes without binary vectors as applied to the unclean 20newsgroups data: ',accuracy_score(nb.predict(vectors_test), data_test_y))\n","\n","\n","# =============================================================================\n","# Naive Bayes without binary vectors as applied to the cleaned 20newsgroups data\n","# =============================================================================\n","data_clean = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n","\n","data_clean_train_X, data_test_X = train_test_split(data_clean.data, random_state=1 )\n","data_clean_train_y, data_test_y = train_test_split(data_clean.target, random_state=1 )\n","\n","vect_false = CountVectorizer(binary=False)\n","vect_false_train=vect_false.fit_transform(data_train_X)\n","vect_false_test=vect_false.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vect_false_train, data_train_y)\n","\n","#print the accuracy score\n","print('The accuracy score for the Naive Bayes without binary vectors as applied to the cleaned 20newsgroups data: ',accuracy_score(nb.predict(vect_false_test), data_test_y))\n","\n","\n","\n","\n","\n","# =============================================================================\n","# The script is getting hung up on the SVC fitting...\n","# =============================================================================\n","#from sklearn.svm import SVC\n","#from sklearn.feature_extraction.text import TfidfVectorizer\n","#\n","## getting 20newsgroups dataset\n","#data = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n","#\n","#data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n","#data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n","#\n","#vectorizer = TfidfVectorizer(binary=True)\n","#vectors_train=vectorizer.fit_transform(data_train_X)\n","#vectors_test=vectorizer.transform(data_test_X)\n","#\n","#svm = SVC(gamma='auto')\n","#svm.fit(vectors_train, data_train_y)\n","#\n","#\n","#print(confusion_matrix(svm.predict(vectors_test), data_test_y))\n","#print(accuracy_score(svm.predict(vectors_test), data_test_y))\n","# =============================================================================\n","# I will debug this later.\n","# =============================================================================\n","\n","\n","# =============================================================================\n","# Making the function for getting the sms spam dataset\n","# =============================================================================\n","# It's worth while reading through this function there's useful things here, that\n","# I'm not explicitly covering in class.\n","def fetch_sms_spam():\n","  import requests # requests is a handy http library\n","  import zipfile # a zip library\n","  r=requests.get('http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/smsspamcollection.zip')\n","  # I know of no way to get data out of a zipfile without saving to disk extracting and reading in, sigh\n","  zf=open('smsspam.zip','wb')\n","  zf.write(r.content)\n","  zip_ref = zipfile.ZipFile('smsspam.zip', 'r')\n","  zip_ref.extractall('smsspam')\n","  zip_ref.close()\n","  zf.close\n","  sms_file=open('smsspam/SMSSpamCollection.txt','r')\n","  #object to return\n","  data = {'data':[], 'target':[], 'target_classes': ['ham', 'spam']}\n","  # First tab splits the class and the SMS message\n","  # There's an argument to be made I should use the csv library and use\n","  # delimiter = '\\t', I can't argue with that - it's a good idea. I did it this\n","  # way for pedogogy.\n","  for line in sms_file:\n","    idx = line.find('\\t')\n","    target = line[:idx]\n","    doc = line[idx+1:]\n","    data['data'] += [doc]\n","    if target == 'ham': data['target'] += [0]\n","    else: data['target'] += [1]\n","  sms_file.close()\n","  return data\n","\n","data1 = fetch_sms_spam()\n","\n","\n","# =============================================================================\n","# SVC model with binary vectors as applied to the sms spam data\n","# =============================================================================\n","data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n","data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n","\n","vectorizer = CountVectorizer(binary=True)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","svm = SVC(gamma='auto')\n","svm.fit(vectors_train, data_train_y)\n","\n","#print the accuracy score\n","print('\\nThe accuracy score for the SVC model with binary vectors as applied to the sms spam data: ',accuracy_score(svm.predict(vectors_test), data_test_y))\n","\n","\n","# =============================================================================\n","# SVC model without binary vectors as applied to the sms spam data\n","# =============================================================================\n","from sklearn.svm import SVC\n","\n","data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n","data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n","\n","vectorizer = CountVectorizer(binary=False)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","svm = SVC(gamma='auto')\n","svm.fit(vectors_train, data_train_y)\n","\n","#print the accuracy score\n","print('The accuracy score for the SVC model without binary vectors as applied to the sms spam data: ',accuracy_score(svm.predict(vectors_test), data_test_y))\n","\n","\n","# =============================================================================\n","# Naive Bayes model with binary vectors as applied to the sms spam data\n","# =============================================================================\n","data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n","data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n","\n","vectorizer = CountVectorizer(binary=True)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vectors_train, data_train_y)\n","\n","#print the accuracy score\n","print('The accuracy score for the NB model with binary vectors as applied to the sms spam data: ',accuracy_score(nb.predict(vectors_test), data_test_y))\n","\n","\n","# =============================================================================\n","# Naive Bayes model without binary vectors as applied to the sms spam data\n","# =============================================================================\n","vectorizer = CountVectorizer(binary=False)\n","vectors_train=vectorizer.fit_transform(data_train_X)\n","vectors_test=vectorizer.transform(data_test_X)\n","\n","nb = MultinomialNB(alpha=.1)\n","nb.fit(vectors_train, data_train_y)\n","\n","#print the accuracy score\n","print('The accuracy score for the NB model without binary vectors as applied to the sms spam data: ',accuracy_score(nb.predict(vectors_test), data_test_y))"],"execution_count":28,"outputs":[{"output_type":"stream","text":["The accuracy score for the Naive Bayes with binary vectors as applied to the unclean 20newsgroups data is:  0.8843378607809848\n","The accuracy score for the Naive Bayes with binary vectors as applied to the cleaned 20newsgroups data is:  0.7425721561969439\n","The accuracy score for the Naive Bayes without binary vectors as applied to the unclean 20newsgroups data:  0.8726655348047538\n","The accuracy score for the Naive Bayes without binary vectors as applied to the cleaned 20newsgroups data:  0.7292020373514432\n","\n","The accuracy score for the SVC model with binary vectors as applied to the sms spam data:  0.8694404591104734\n","The accuracy score for the SVC model without binary vectors as applied to the sms spam data:  0.8694404591104734\n","The accuracy score for the NB model with binary vectors as applied to the sms spam data:  0.9885222381635581\n","The accuracy score for the NB model without binary vectors as applied to the sms spam data:  0.9892395982783357\n"],"name":"stdout"}]}]}