{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pqFCAkTyeMMw"
   },
   "source": [
    "# Homework Problem 1\n",
    "- Note the data contains many headers, quotes and headers.  These create things that are easy to idenitify. You can remove this from the documents using  data_train = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes')\n",
    "- What happens to the performance of the model?  Is this to be expected?  Was it a good idea?\n",
    "- I used a CountVectorizer(binary=True)\n",
    "- What happens to the vectors if you set binary=False?  What happens to the performance?\n",
    "- Try training an svm with a linear kernel using binary vectors, count vectors, also apply a tfidf transform to the data.  Compare performance with Naive Bayes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5985,
     "status": "ok",
     "timestamp": 1551643883645,
     "user": {
      "displayName": "Mattias Herrfurth",
      "photoUrl": "",
      "userId": "10294660052558722971"
     },
     "user_tz": 300
    },
    "id": "pekQtaQ1eFS_",
    "outputId": "bfe3217f-a7e2-4668-8a4c-9d8d388c22ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[184   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0   0\n",
      "    0  17]\n",
      " [  0 220  36   5   3  22   0   3   0   0   0   7   7   2   1   2   0   0\n",
      "    0   0]\n",
      " [  0   0  81   0   2   0   1   0   0   0   0   0   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   7  97 231  10   6   8   0   0   2   0   1   9   3   1   1   0   0\n",
      "    0   0]\n",
      " [  0   3   6  10 211   0   4   0   1   0   0   0   6   0   1   1   0   0\n",
      "    0   0]\n",
      " [  0   4  23   1   0 194   0   0   0   0   1   2   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  1   1   2   6   3   1 214   4   4   2   0   0   1   1   1   0   0   0\n",
      "    0   0]\n",
      " [  1   0   3   0   0   0  10 240   1   0   0   0   2   1   0   0   0   0\n",
      "    0   1]\n",
      " [  0   0   0   0   0   1   3  10 244   0   1   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   2   0   0 221   2   0   0   0   1   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   1   0   0   1   0   0   1 255   0   0   0   0   1   0   0\n",
      "    0   0]\n",
      " [  0   2   5   0   1   1   2   1   0   0   0 244   1   0   0   0   2   0\n",
      "    0   0]\n",
      " [  0   4   5   8   4   2   8   3   1   0   1   0 202   3   3   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   1   1   2   0   0   0   0   0 233   0   5   0   1\n",
      "    1   0]\n",
      " [  0   0   2   0   0   2   1   0   0   0   0   0   0   4 247   0   0   0\n",
      "    1   0]\n",
      " [  3   0   0   0   0   0   0   0   0   0   0   0   0   0   0 228   0   1\n",
      "    1   8]\n",
      " [  0   0   0   0   0   0   2   2   0   0   1   1   0   0   0   0 225   1\n",
      "   16   4]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   1 220\n",
      "    2   0]\n",
      " [  0   1   2   0   0   0   0   0   0   0   1   2   1   2   0   1   5   1\n",
      "  174   3]\n",
      " [ 10   0   1   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0\n",
      "    2  99]]\n",
      "0.8843378607809848\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# getting 20newsgroups dataset\n",
    "data = datasets.fetch_20newsgroups(subset='all')\n",
    "\n",
    "data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n",
    "data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n",
    "\n",
    "# vectorizer = CountVectorizer(binary=False)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print a confusion matrix and accuracy\n",
    "print(confusion_matrix(nb.predict(vectors_test), data_test_y))\n",
    "print(accuracy_score(nb.predict(vectors_test), data_test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12926,
     "status": "ok",
     "timestamp": 1551643890601,
     "user": {
      "displayName": "Mattias Herrfurth",
      "photoUrl": "",
      "userId": "10294660052558722971"
     },
     "user_tz": 300
    },
    "id": "QdwLlVh2fm2R",
    "outputId": "3aba6f97-1e8f-4ee2-9a4a-cf37fc36ebfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[138   1   2   1   1   1   0   2   3   0   3   3   0   2   2   7   4   5\n",
      "    9  21]\n",
      " [  1 181  38   4   2  27   3   2   0   0   0   6   8   3   7   2   0   2\n",
      "    0   0]\n",
      " [  0   0  34   0   3   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0  11 104 219  17   8  14   0   2   2   0   0  14   2   2   0   0   0\n",
      "    0   0]\n",
      " [  0  12  14   9 180   4  10   0   2   0   1   1   8   1   1   1   0   1\n",
      "    1   0]\n",
      " [  0  10  37   2   2 175   0   1   0   0   1   2   0   0   2   0   0   1\n",
      "    0   0]\n",
      " [  0   1   2   4   4   2 179   6   3   4   0   0   3   2   0   0   0   0\n",
      "    0   1]\n",
      " [  2   0   0   2   4   1  15 214  16   0   1   0   4   2   3   0   1   0\n",
      "    1   2]\n",
      " [  3   1   4   0   0   1   1   6 191   1   3   1   2   2   0   1   2   1\n",
      "    2   0]\n",
      " [  5   7   9   7   8   2   4  12   9 209  13  11   7   7  10   5   9   6\n",
      "    5   5]\n",
      " [  0   1   1   0   0   0   3   0   1   1 226   0   0   0   1   0   0   0\n",
      "    1   0]\n",
      " [  4   5   4   2   2   5   5   1   1   1   1 203   4   0   3   0   8   2\n",
      "    1   0]\n",
      " [  0   5   5   8   7   0  14   6   4   0   1   3 171   2   3   1   1   0\n",
      "    0   1]\n",
      " [  1   0   0   2   2   0   0   2   2   2   1   2   0 209   2   2   0   0\n",
      "    7   1]\n",
      " [  0   2   1   0   0   2   1   1   3   0   0   3   3   3 205   1   1   0\n",
      "    1   1]\n",
      " [ 22   2   4   0   2   0   3   1   1   1   3   3   0   6   3 214   1   2\n",
      "    3  39]\n",
      " [  5   1   0   1   0   1   3   5   6   2   4   8   2   1   1   2 185   4\n",
      "   32   8]\n",
      " [  3   1   0   0   0   0   0   1   1   0   1   0   0   1   2   2   2 191\n",
      "    3   1]\n",
      " [  3   1   2   1   0   0   2   5   4   3   2   9   0   6   7   3  13   9\n",
      "  126   3]\n",
      " [ 12   0   2   0   0   1   0   0   2   0   1   2   3   0   2   2   6   0\n",
      "    5  49]]\n",
      "0.7425721561969439\n"
     ]
    }
   ],
   "source": [
    "# getting cleaned data for training\n",
    "data_clean = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n",
    "\n",
    "data_clean_train_X, data_test_X = train_test_split(data_clean.data, random_state=1 )\n",
    "data_clean_train_y, data_test_y = train_test_split(data_clean.target, random_state=1 )\n",
    "\n",
    "# vect_clean = CountVectorizer(binary=False)\n",
    "vect_clean = CountVectorizer(binary=True)\n",
    "vect_clean_train=vect_clean.fit_transform(data_train_X)\n",
    "vect_clean_test=vect_clean.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vect_clean_train, data_train_y)\n",
    "\n",
    "#print a confusion matrix and accuracy\n",
    "print(confusion_matrix(nb.predict(vect_clean_test), data_test_y))\n",
    "print(accuracy_score(nb.predict(vect_clean_test), data_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4otPkpWljjj6"
   },
   "source": [
    "# RESULT\n",
    "... it looks like it got worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3UWWIO8skFi_"
   },
   "source": [
    "* I used a CountVectorizer(binary=True)\n",
    "* What happens to the vectors if you set binary=False? What happens to the performance?\n",
    "* Try training an svm with a linear kernel using binary vectors, count vectors, also apply a tfidf transform to the data. Compare performance with Naive Bayes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19912,
     "status": "ok",
     "timestamp": 1551643897599,
     "user": {
      "displayName": "Mattias Herrfurth",
      "photoUrl": "",
      "userId": "10294660052558722971"
     },
     "user_tz": 300
    },
    "id": "XohRvjW8kN0k",
    "outputId": "7c77c92f-204a-46f2-903c-54b6c2018a02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[135   0   2   1   1   0   0   3   2   1   6   2   1   1   6  10   5   7\n",
      "    9  27]\n",
      " [  1 186  45   4   4  31   4   2   1   2   0   3   9   1   6   3   0   2\n",
      "    0   0]\n",
      " [  0   0   8   0   2   0   0   0   0   0   0   0   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   8 104 210  16   5  11   0   2   0   0   0  12   2   0   0   0   0\n",
      "    0   0]\n",
      " [  0  10  17  12 174   4  11   0   2   0   1   1   8   1   1   1   0   0\n",
      "    0   0]\n",
      " [  0  10  44   2   3 174   0   1   0   0   0   3   0   0   1   0   0   1\n",
      "    0   0]\n",
      " [  0   0   2   5   4   1 174   3   2   3   0   0   1   1   0   0   0   0\n",
      "    0   0]\n",
      " [  1   0   0   1   4   2  15 217  16   0   1   1   5   2   3   0   1   0\n",
      "    1   2]\n",
      " [  3   1   1   0   0   1   3   9 190   1   1   1   1   2   2   1   0   0\n",
      "    0   0]\n",
      " [  5   7  10   7   7   1   4  12   9 206  17  11   7   6   9   5   7   5\n",
      "    5   5]\n",
      " [  0   0   1   0   0   0   1   0   1   1 222   1   0   0   0   0   0   0\n",
      "    1   0]\n",
      " [  2   7  10   2   2   3   6   0   1   0   0 200   7   0   2   0   7   3\n",
      "    0   0]\n",
      " [  0   3   5  11  10   1  15   4   3   1   0   3 167   2   3   1   1   0\n",
      "    0   0]\n",
      " [  1   1   1   1   3   1   1   1   3   2   1   1   1 215   2   0   0   0\n",
      "    5   1]\n",
      " [  1   2   1   0   0   2   1   0   2   1   0   3   2   1 202   1   2   0\n",
      "    2   1]\n",
      " [ 22   2   4   2   2   2   2   2   0   1   4   2   0   4   3 210   0   4\n",
      "    4  36]\n",
      " [  5   2   2   2   0   1   4   4   6   1   3   7   3   1   1   1 178   3\n",
      "   31   9]\n",
      " [  5   1   0   0   0   0   0   1   2   2   1   3   0   2   3   4   5 191\n",
      "    3   1]\n",
      " [  4   2   5   2   1   0   5   6   7   4   4  14   1   8  10   3  21   7\n",
      "  131   4]\n",
      " [ 14   0   1   0   1   1   0   0   2   0   1   1   4   0   2   3   6   1\n",
      "    5  46]]\n",
      "0.7292020373514432\n"
     ]
    }
   ],
   "source": [
    "# getting cleaned data for training\n",
    "data_clean = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n",
    "\n",
    "data_clean_train_X, data_test_X = train_test_split(data_clean.data, random_state=1 )\n",
    "data_clean_train_y, data_test_y = train_test_split(data_clean.target, random_state=1 )\n",
    "\n",
    "vect_false = CountVectorizer(binary=False)\n",
    "vect_false_train=vect_false.fit_transform(data_train_X)\n",
    "vect_false_test=vect_false.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vect_false_train, data_train_y)\n",
    "\n",
    "#print a confusion matrix and accuracy\n",
    "print(confusion_matrix(nb.predict(vect_false_test), data_test_y))\n",
    "print(accuracy_score(nb.predict(vect_false_test), data_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bMxynHhRlBdI"
   },
   "source": [
    "# RESULT\n",
    "... it looks like it got worse again????\n",
    "\n",
    "Let's try it on the original..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25228,
     "status": "ok",
     "timestamp": 1551643902930,
     "user": {
      "displayName": "Mattias Herrfurth",
      "photoUrl": "",
      "userId": "10294660052558722971"
     },
     "user_tz": 300
    },
    "id": "QG-bTeyLlQAz",
    "outputId": "1281e358-edda-45ee-e4b0-185ccf34ef57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[183   0   1   0   0   0   0   0   0   0   0   0   0   0   0   2   0   0\n",
      "    0  16]\n",
      " [  0 220  43   5   7  29   1   2   0   0   0   3   7   1   2   1   0   1\n",
      "    0   0]\n",
      " [  0   0  39   0   2   0   1   0   0   0   0   0   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  0   7 101 232   9   5   5   0   0   0   0   0  13   2   0   1   1   0\n",
      "    0   0]\n",
      " [  0   3  25  11 209   1   4   0   1   0   0   0   5   0   1   1   0   1\n",
      "    0   0]\n",
      " [  0   5  30   1   0 189   0   0   0   0   0   2   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  1   2   5   5   2   1 213   3   4   1   0   1   0   2   1   0   0   0\n",
      "    0   0]\n",
      " [  1   0   3   1   0   0  12 241   1   0   0   0   2   0   0   1   0   0\n",
      "    0   1]\n",
      " [  0   0   2   0   0   1   2  14 244   0   1   0   1   2   0   1   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   2   0   0 221   4   0   0   0   2   0   0   0\n",
      "    0   1]\n",
      " [  0   0   0   1   0   0   1   0   0   1 255   0   0   0   0   1   0   0\n",
      "    0   0]\n",
      " [  0   2   3   0   0   1   2   0   0   1   0 248   0   1   0   0   1   1\n",
      "    2   0]\n",
      " [  0   2   7   6   4   1  10   4   1   2   0   0 200   3   2   0   0   0\n",
      "    0   0]\n",
      " [  0   0   0   0   0   0   2   0   0   0   0   0   0 231   1   3   0   0\n",
      "    0   0]\n",
      " [  0   1   0   0   0   2   0   0   0   0   0   0   0   4 247   0   0   0\n",
      "    2   0]\n",
      " [  3   0   0   0   0   0   0   0   0   0   0   0   0   0   0 231   0   1\n",
      "    1   9]\n",
      " [  0   0   0   0   0   0   2   1   0   0   1   1   0   1   0   0 225   1\n",
      "   19   4]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1   0 219\n",
      "    3   0]\n",
      " [  0   0   3   0   1   0   0   0   0   0   1   2   0   2   0   0   6   0\n",
      "  167   3]\n",
      " [ 11   0   1   0   0   0   0   0   0   0   0   0   1   0   0   0   0   0\n",
      "    3  98]]\n",
      "0.8726655348047538\n"
     ]
    }
   ],
   "source": [
    "# getting 20newsgroups dataset\n",
    "data = datasets.fetch_20newsgroups(subset='all')\n",
    "\n",
    "data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n",
    "data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n",
    "\n",
    "vectorizer = CountVectorizer(binary=False)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print a confusion matrix and accuracy\n",
    "print(confusion_matrix(nb.predict(vectors_test), data_test_y))\n",
    "print(accuracy_score(nb.predict(vectors_test), data_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rxyks3DllXyA"
   },
   "source": [
    "# RESULT\n",
    "Also worse, but not by much (0.8843 down to 0.8727)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4zEIUQ6hluW5"
   },
   "source": [
    "* Try training an svm with a linear kernel using binary vectors, count vectors, also apply a tfidf transform to the data. Compare performance with Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAoZRulIlwhB"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fjZhs1lZ1vjI"
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # getting 20newsgroups dataset\n",
    "# data = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n",
    "\n",
    "# data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n",
    "# data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n",
    "\n",
    "# vectorizer = TfidfVectorizer(binary=True)\n",
    "# vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "# vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "# svm = SVC(gamma='auto')\n",
    "# svm.fit(vectors_train, data_train_y)\n",
    "\n",
    "# #print a confusion matrix and accuracy\n",
    "# print(confusion_matrix(svm.predict(vectors_test), data_test_y))\n",
    "# print(accuracy_score(svm.predict(vectors_test), data_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OEh38X8FE-7r"
   },
   "source": [
    "# RESULT\n",
    "* This actually took really long, and after researching a bit on the web it seems this is to be expected...\n",
    "* I just let it keep running, i hope nothing bad happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "88cMu4BBeEIY"
   },
   "source": [
    "# Homework Problem 2\n",
    "- Use the fetch_sms_spam() function below to get the sms spam data set.\n",
    "- More info: [Here](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)\n",
    "- Apply the vectorization and classification tasks above to  the data.\n",
    "- Which gave the best performance (use a confusionmatrix and accuraccy score to help decide this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q93BVO8XMHnQ"
   },
   "outputs": [],
   "source": [
    "# It's worth while reading through this function there's useful things here, that\n",
    "# I'm not explicitly covering in class.\n",
    "def fetch_sms_spam():\n",
    "  import requests # requests is a handy http library\n",
    "  import zipfile # a zip library\n",
    "  r=requests.get('http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/smsspamcollection.zip')\n",
    "  # I know of no way to get data out of a zipfile without saving to disk extracting and reading in, sigh\n",
    "  zf=open('smsspam.zip','wb')\n",
    "  zf.write(r.content)\n",
    "  zip_ref = zipfile.ZipFile('smsspam.zip', 'r')\n",
    "  zip_ref.extractall('smsspam')\n",
    "  zip_ref.close()\n",
    "  zf.close\n",
    "  sms_file=open('smsspam/SMSSpamCollection.txt','r')\n",
    "  #object to return\n",
    "  data = {'data':[], 'target':[], 'target_classes': ['ham', 'spam']}\n",
    "  # First tab splits the class and the SMS message\n",
    "  # There's an argument to be made I should use the csv library and use\n",
    "  # delimiter = '\\t', I can't argue with that - it's a good idea. I did it this\n",
    "  # way for pedogogy.\n",
    "  for line in sms_file:\n",
    "    idx = line.find('\\t')\n",
    "    target = line[:idx]\n",
    "    doc = line[idx+1:]\n",
    "    data['data'] += [doc]\n",
    "    if target == 'ham': data['target'] += [0]\n",
    "    else: data['target'] += [1]\n",
    "  sms_file.close()\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROJZrR_kt_IJ"
   },
   "outputs": [],
   "source": [
    "data1 = fetch_sms_spam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28976,
     "status": "ok",
     "timestamp": 1551643906718,
     "user": {
      "displayName": "Mattias Herrfurth",
      "photoUrl": "",
      "userId": "10294660052558722971"
     },
     "user_tz": 300
    },
    "id": "rEo8XyHQxHb-",
    "outputId": "3be6e1e4-a58c-4b2e-fa94-2ad4e57c2b83"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1212  182]\n",
      " [   0    0]]\n",
      "0.8694404591104734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n",
    "data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print a confusion matrix and accuracy\n",
    "print(confusion_matrix(svm.predict(vectors_test), data_test_y))\n",
    "print(accuracy_score(svm.predict(vectors_test), data_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RPGi3vZd97NP"
   },
   "source": [
    "# RESULT\n",
    "\n",
    "* Not entirely sure what to make of that...\n",
    "\n",
    "* Accuracy score looks decent, at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30401,
     "status": "ok",
     "timestamp": 1551643908150,
     "user": {
      "displayName": "Mattias Herrfurth",
      "photoUrl": "",
      "userId": "10294660052558722971"
     },
     "user_tz": 300
    },
    "id": "2gbFu-o0_opf",
    "outputId": "1edc0297-4768-4f72-816b-18e09f2ac637"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1212  182]\n",
      " [   0    0]]\n",
      "0.8694404591104734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n",
    "data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n",
    "\n",
    "vectorizer = CountVectorizer(binary=False)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print a confusion matrix and accuracy\n",
    "print(confusion_matrix(svm.predict(vectors_test), data_test_y))\n",
    "print(accuracy_score(svm.predict(vectors_test), data_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TE6iDWJV_vKQ"
   },
   "source": [
    "# RESULT\n",
    "\n",
    "* vectors being binary or not doesn't matter for SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30507,
     "status": "ok",
     "timestamp": 1551643908265,
     "user": {
      "displayName": "Mattias Herrfurth",
      "photoUrl": "",
      "userId": "10294660052558722971"
     },
     "user_tz": 300
    },
    "id": "5fM5Lgxc-HvP",
    "outputId": "348f10bf-d6e7-410c-8d6e-8e56254e5104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1204    7]\n",
      " [   8  175]]\n",
      "0.9892395982783357\n"
     ]
    }
   ],
   "source": [
    "data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n",
    "data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n",
    "\n",
    "vectorizer = CountVectorizer(binary=False)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print a confusion matrix and accuracy\n",
    "print(confusion_matrix(nb.predict(vectors_test), data_test_y))\n",
    "print(accuracy_score(nb.predict(vectors_test), data_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBmz9a7p-eFA"
   },
   "source": [
    "# RESULT\n",
    "* Again, confusion matrix is a bit odd...\n",
    "* But accuracy score is way better with Naive Bayes!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30618,
     "status": "ok",
     "timestamp": 1551643908383,
     "user": {
      "displayName": "Mattias Herrfurth",
      "photoUrl": "",
      "userId": "10294660052558722971"
     },
     "user_tz": 300
    },
    "id": "n1jDDEZC-p0I",
    "outputId": "15fa620b-b5ca-492c-f36c-368c630feff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1203    7]\n",
      " [   9  175]]\n",
      "0.9885222381635581\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print a confusion matrix and accuracy\n",
    "print(confusion_matrix(nb.predict(vectors_test), data_test_y))\n",
    "print(accuracy_score(nb.predict(vectors_test), data_test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i7klzvIn-5__"
   },
   "source": [
    "# RESULT\n",
    "* nearly identical accuracy score no matter if binary or not\n",
    "\n",
    "# CONCLUSION\n",
    "\n",
    "* out of previous tests of various prediction models, it was found that the accuracy scores are as follow:\n",
    "  * SVC = 0.8694\n",
    "  * NB w/ Binary Vectors = 0.9885\n",
    "  * NB w/o Binary Vectors = 0.9892\n",
    "* It looks like the Naive Bayes without Binary Vectors performed the best for the spam detection data!\n",
    "* The following code produces only the accuracy score for all the models created in this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29649,
     "status": "ok",
     "timestamp": 1551644244456,
     "user": {
      "displayName": "Mattias Herrfurth",
      "photoUrl": "",
      "userId": "10294660052558722971"
     },
     "user_tz": 300
    },
    "id": "lkp9m12FI7rh",
    "outputId": "abb156c1-564e-46d1-ea17-05a682b35d1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for the Naive Bayes with binary vectors as applied to the unclean 20newsgroups data is:  0.8843378607809848\n",
      "The accuracy score for the Naive Bayes with binary vectors as applied to the cleaned 20newsgroups data without headers, footers, and quotes:  0.7425721561969439\n",
      "The accuracy score for the Naive Bayes without binary vectors as applied to the cleaned 20newsgroups data:  0.7292020373514432\n",
      "The accuracy score for the Naive Bayes without binary vectors as applied to the unclean 20newsgroups data:  0.8726655348047538\n",
      "The accuracy score for the SVC model with binary vectors as applied to the sms spam data:  0.8694404591104734\n",
      "The accuracy score for the SVC model without binary vectors as applied to the sms spam data:  0.8694404591104734\n",
      "The accuracy score for the Naive Bayes model with binary vectors as applied to the sms spam data:  0.9885222381635581\n",
      "The accuracy score for the Naive Bayes model without binary vectors as applied to the sms spam data:  0.9892395982783357\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Mar  3 14:43:55 2019\n",
    "\n",
    "@author: matti\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# Naive Bayes with binary vectors as applied to the unclean 20newsgroups data\n",
    "# =============================================================================\n",
    "# getting 20newsgroups dataset\n",
    "data = datasets.fetch_20newsgroups(subset='all')\n",
    "\n",
    "data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n",
    "data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n",
    "\n",
    "# vectorizer = CountVectorizer(binary=False)\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print the accuracy score\n",
    "print('The accuracy score for the Naive Bayes with binary vectors as applied to the unclean 20newsgroups data is: ',accuracy_score(nb.predict(vectors_test), data_test_y))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Naive Bayes with binary vectors as applied to the cleaned 20newsgroups data without headers, footers, and quotes\n",
    "# =============================================================================\n",
    "# getting cleaned data for training\n",
    "data_clean = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n",
    "\n",
    "data_clean_train_X, data_test_X = train_test_split(data_clean.data, random_state=1 )\n",
    "data_clean_train_y, data_test_y = train_test_split(data_clean.target, random_state=1 )\n",
    "\n",
    "# vect_clean = CountVectorizer(binary=False)\n",
    "vect_clean = CountVectorizer(binary=True)\n",
    "vect_clean_train=vect_clean.fit_transform(data_train_X)\n",
    "vect_clean_test=vect_clean.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vect_clean_train, data_train_y)\n",
    "\n",
    "#print the accuracy score\n",
    "print('The accuracy score for the Naive Bayes with binary vectors as applied to the cleaned 20newsgroups data without headers, footers, and quotes: ',accuracy_score(nb.predict(vect_clean_test), data_test_y))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Naive Bayes without binary vectors as applied to the cleaned 20newsgroups data\n",
    "# =============================================================================\n",
    "data_clean = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n",
    "\n",
    "data_clean_train_X, data_test_X = train_test_split(data_clean.data, random_state=1 )\n",
    "data_clean_train_y, data_test_y = train_test_split(data_clean.target, random_state=1 )\n",
    "\n",
    "vect_false = CountVectorizer(binary=False)\n",
    "vect_false_train=vect_false.fit_transform(data_train_X)\n",
    "vect_false_test=vect_false.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vect_false_train, data_train_y)\n",
    "\n",
    "#print the accuracy score\n",
    "print('The accuracy score for the Naive Bayes without binary vectors as applied to the cleaned 20newsgroups data: ',accuracy_score(nb.predict(vect_false_test), data_test_y))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Naive Bayes without binary vectors as applied to the unclean 20newsgroups data\n",
    "# =============================================================================\n",
    "# getting 20newsgroups dataset\n",
    "data = datasets.fetch_20newsgroups(subset='all')\n",
    "\n",
    "data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n",
    "data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n",
    "\n",
    "vectorizer = CountVectorizer(binary=False)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print the accuracy score\n",
    "print('The accuracy score for the Naive Bayes without binary vectors as applied to the unclean 20newsgroups data: ',accuracy_score(nb.predict(vectors_test), data_test_y))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# The script is getting hung up on the SVC fitting...\n",
    "# =============================================================================\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#\n",
    "## getting 20newsgroups dataset\n",
    "#data = datasets.fetch_20newsgroups(subset='all',remove=('headers','footers','quotes'))\n",
    "#\n",
    "#data_train_X, data_test_X = train_test_split(data.data, random_state=1 )\n",
    "#data_train_y, data_test_y = train_test_split(data.target, random_state=1 )\n",
    "#\n",
    "#vectorizer = TfidfVectorizer(binary=True)\n",
    "#vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "#vectors_test=vectorizer.transform(data_test_X)\n",
    "#\n",
    "#svm = SVC(gamma='auto')\n",
    "#svm.fit(vectors_train, data_train_y)\n",
    "#\n",
    "#\n",
    "#print(confusion_matrix(svm.predict(vectors_test), data_test_y))\n",
    "#print(accuracy_score(svm.predict(vectors_test), data_test_y))\n",
    "# =============================================================================\n",
    "# I will debug this later.\n",
    "# =============================================================================\n",
    "\n",
    "# =============================================================================\n",
    "# Making the function for getting the sms spam dataset\n",
    "# =============================================================================\n",
    "# It's worth while reading through this function there's useful things here, that\n",
    "# I'm not explicitly covering in class.\n",
    "def fetch_sms_spam():\n",
    "  import requests # requests is a handy http library\n",
    "  import zipfile # a zip library\n",
    "  r=requests.get('http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/smsspamcollection.zip')\n",
    "  # I know of no way to get data out of a zipfile without saving to disk extracting and reading in, sigh\n",
    "  zf=open('smsspam.zip','wb')\n",
    "  zf.write(r.content)\n",
    "  zip_ref = zipfile.ZipFile('smsspam.zip', 'r')\n",
    "  zip_ref.extractall('smsspam')\n",
    "  zip_ref.close()\n",
    "  zf.close\n",
    "  sms_file=open('smsspam/SMSSpamCollection.txt','r')\n",
    "  #object to return\n",
    "  data = {'data':[], 'target':[], 'target_classes': ['ham', 'spam']}\n",
    "  # First tab splits the class and the SMS message\n",
    "  # There's an argument to be made I should use the csv library and use\n",
    "  # delimiter = '\\t', I can't argue with that - it's a good idea. I did it this\n",
    "  # way for pedogogy.\n",
    "  for line in sms_file:\n",
    "    idx = line.find('\\t')\n",
    "    target = line[:idx]\n",
    "    doc = line[idx+1:]\n",
    "    data['data'] += [doc]\n",
    "    if target == 'ham': data['target'] += [0]\n",
    "    else: data['target'] += [1]\n",
    "  sms_file.close()\n",
    "  return data\n",
    "\n",
    "data1 = fetch_sms_spam()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SVC model with binary vectors as applied to the sms spam data\n",
    "# =============================================================================\n",
    "data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n",
    "data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print the accuracy score\n",
    "print('The accuracy score for the SVC model with binary vectors as applied to the sms spam data: ',accuracy_score(svm.predict(vectors_test), data_test_y))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SVC model without binary vectors as applied to the sms spam data\n",
    "# =============================================================================\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n",
    "data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n",
    "\n",
    "vectorizer = CountVectorizer(binary=False)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "svm = SVC(gamma='auto')\n",
    "svm.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print the accuracy score\n",
    "print('The accuracy score for the SVC model without binary vectors as applied to the sms spam data: ',accuracy_score(svm.predict(vectors_test), data_test_y))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Naive Bayes model with binary vectors as applied to the sms spam data\n",
    "# =============================================================================\n",
    "data_train_X, data_test_X = train_test_split(data1['data'], random_state=1 )\n",
    "data_train_y, data_test_y = train_test_split(data1['target'], random_state=1 )\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print the accuracy score\n",
    "print('The accuracy score for the Naive Bayes model with binary vectors as applied to the sms spam data: ',accuracy_score(nb.predict(vectors_test), data_test_y))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Naive Bayes model without binary vectors as applied to the sms spam data\n",
    "# =============================================================================\n",
    "vectorizer = CountVectorizer(binary=False)\n",
    "vectors_train=vectorizer.fit_transform(data_train_X)\n",
    "vectors_test=vectorizer.transform(data_test_X)\n",
    "\n",
    "nb = MultinomialNB(alpha=.1)\n",
    "nb.fit(vectors_train, data_train_y)\n",
    "\n",
    "#print the accuracy score\n",
    "print('The accuracy score for the Naive Bayes model without binary vectors as applied to the sms spam data: ',accuracy_score(nb.predict(vectors_test), data_test_y))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of herrfurth_hw4.ipynb",
   "provenance": [
    {
     "file_id": "1J7t-aV2KkHCXGYNHyuoWw_unP-0A_2pe",
     "timestamp": 1551643738221
    },
    {
     "file_id": "1BPrDHd3bMlOn3o_WJksg3z_hhH8a-EU6",
     "timestamp": 1551311884842
    },
    {
     "file_id": "1ekkdt4NJRpu3Farh-cUQZmoP3q1eRKWG",
     "timestamp": 1536031133585
    },
    {
     "file_id": "1DziSbVqaWErcHDGNfhjupryp5jZFd-t2",
     "timestamp": 1535684058223
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
